{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolve Schema\n",
    "\n",
    "**input**\n",
    "```\n",
    "+schema_path: str\n",
    "```\n",
    "\n",
    "**methods**\n",
    "```\n",
    "+read_json(schema_path: str): dict\n",
    "+split_json(schema: dict): list\n",
    "+resolver(entity1: dict, entity2: dict): dict\n",
    "+resolve_defs(terms: dict, defs: dict) : dict\n",
    "+ node_order(schema: dict): list\n",
    "+resolve_nodes(nodeList: list, splitJsonList: list): list\n",
    "+recombine_nodes(resolvedList: list) : dict\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    filename=f\"../logs/{datetime.now()}.log\",\n",
    "    filemode='x',\n",
    "    force=True  # Ensures custom configuration takes effect\n",
    ")\n",
    "\n",
    "logging.info(\"This log should appear in logs.log\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gen3_data_validator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in xlsx data and writing to json\n",
    "- xlsx data comes from xlsx manifest file created from acdc_submission_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResolverClass = gen3_data_validator.ResolveSchema(schema_path = \"../schema/gen3_test_schema.json\")\n",
    "xlsxData = gen3_data_validator.ParseXlsxMetadata(xlsx_path = \"../data/restricted/ausdiab_lipid_manifest.xlsx\", skip_rows=1)\n",
    "xlsxData.write_dict_to_json(output_dir=\"../data/restricted/ausdiab_lipid_metadata/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = gen3_data_validator.ParseData(data_folder_path = \"../data/fail\")\n",
    "Resolver = gen3_data_validator.ResolveSchema(schema_path = \"../schema/gen3_test_schema.json\")\n",
    "Linkage = gen3_data_validator.TestLinkage(schema_resolver = Resolver, data_parser = Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also bypass the data attribute in the linkage class and input your own data_map and config_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Linkage class which has the resolved schema with custom data and config\n",
    "config_map = {\n",
    "    \"samples\": {\"primary_key\": \"sample_id\", \"foreign_key\": \"subject_id\"},\n",
    "    \"files\": {\"primary_key\": \"file_id\", \"foreign_key\": \"sample_id\"},\n",
    "    \"subjects\": {\"primary_key\": \"subject_id\", \"foreign_key\": \"project_id\"},\n",
    "    \"project\": {\"primary_key\": \"project_id\", \"foreign_key\": None}\n",
    "}\n",
    "\n",
    "data_map = {\n",
    "    \"samples\": [\n",
    "        {\"sample_id\": \"sample_1\", \"subject_id\": \"subject_9\"},\n",
    "        {\"sample_id\": \"sample_2\", \"subject_id\": \"subject_3\"},  # Invalid FK\n",
    "        {\"sample_id\": \"sample_3\", \"subject_id\": \"subject_4\"}, # Invalid FK\n",
    "        {\"sample_id\": \"sample_4\", \"subject_id\": \"subject_5\"} # Invalid FK\n",
    "    ],\n",
    "    \"files\": [\n",
    "        {\"file_id\": \"file_1\", \"sample_id\": \"sample_1\"},\n",
    "        {\"file_id\": \"file_2\", \"sample_id\": \"sample_27\"}  # Invalid FK\n",
    "    ],\n",
    "    \"subjects\": [\n",
    "        {\"subject_id\": \"subject_1\", \"project_id\": \"project_1\"},  \n",
    "        {\"subject_id\": \"subject_2\", \"project_id\": \"project_2\"}, # Missing project 2\n",
    "    ],\n",
    "    \"project\": [\n",
    "        {\"project_id\": \"project_1\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "Linkage.validate_links(data_map, config_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# validation prototype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jsonschema import Draft4Validator\n",
    "from functools import wraps\n",
    "from datetime import datetime\n",
    "from functools import wraps\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import json\n",
    "import uuid\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "class Validate:\n",
    "    \n",
    "    def __init__(self, data_map, resolved_schema):\n",
    "        self.data_map = data_map\n",
    "        self.resolved_schema = resolved_schema\n",
    "        logging.info(\"Initializing Validate class with data map and resolved schema.\")\n",
    "        self.validation_result = self.validate_schema(self.data_map, self.resolved_schema)\n",
    "    \n",
    "    \n",
    "    def validate_object(self, obj, idx, validator) -> list:\n",
    "        \"\"\"\n",
    "        Validates a single JSON object against a provided JSON schema validator.\n",
    "\n",
    "        Parameters:\n",
    "        - obj (dict): The JSON object to validate.\n",
    "        - idx (int): The index of the object in the dataset.\n",
    "        - validator (Draft4Validator): The JSON schema validator to use for validation.\n",
    "\n",
    "        Returns:\n",
    "        - list: A list of dictionaries containing validation results and log messages.\n",
    "        \"\"\"\n",
    "        validation_results = []\n",
    "        try:\n",
    "            errors = list(validator.iter_errors(obj))\n",
    "            logging.debug(f\"Object at index {idx} validated with {len(errors)} errors.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during object validation at index {idx}: {e}\")\n",
    "            return validation_results\n",
    "\n",
    "        if len(errors[1:]) == 0:\n",
    "            result = {\n",
    "                \"index\": idx,\n",
    "                \"validation_result\": \"PASS\",\n",
    "                \"invalid_key\": None,\n",
    "                \"schema_path\": None,\n",
    "                \"validator\": None,\n",
    "                \"validator_value\": None,\n",
    "                \"validation_error\": None\n",
    "            }\n",
    "            validation_results.append(result)\n",
    "        else:\n",
    "            for error in errors[1:]:\n",
    "                invalid_key = \".\".join(str(k) for k in error.path) if error.path else \"root\"\n",
    "                schema_path = \".\".join(str(k) for k in error.schema_path)\n",
    "\n",
    "                result = {\n",
    "                    \"index\": idx,\n",
    "                    \"validation_result\": \"FAIL\",\n",
    "                    \"invalid_key\": invalid_key,\n",
    "                    \"schema_path\": schema_path,\n",
    "                    \"validator\": error.validator,\n",
    "                    \"validator_value\": error.validator_value,\n",
    "                    \"validation_error\": error.message\n",
    "                }\n",
    "                validation_results.append(result)\n",
    "\n",
    "        return validation_results\n",
    "\n",
    "    def validate_schema(self, data_map: dict, resolved_schema: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Takes in a dictionary of data, where the key is the entity name, and the value is a list of jsons containing the data.\n",
    "        The function then validates the data against the resolved schema.\n",
    "        \n",
    "        Args:\n",
    "        - data_map (dict): A dictionary where keys are entity names and values are lists of JSON objects to be validated.\n",
    "        - resolved_schema (dict): A dict of resolved JSON schema objects to validate against.\n",
    "\n",
    "        Returns:\n",
    "        - dict: A dictionary containing validation results for each entity.\n",
    "        \"\"\"\n",
    "        validation_results = {}\n",
    "        \n",
    "        try:\n",
    "            data_nodes = list(data_map.keys())\n",
    "            logging.info(f\"Data nodes: {data_nodes}\")\n",
    "            schema_keys = [key[:-5] if key.endswith('.yaml') else key for key in resolved_schema.keys()]\n",
    "            logging.info(f\"Schema keys: {schema_keys}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error accessing data or schema keys: {e}\")\n",
    "            return validation_results\n",
    "        \n",
    "        for node in data_nodes:\n",
    "            if node not in schema_keys:\n",
    "                logging.warning(f\"Warning: {node} not found in resolved schema keys.\")\n",
    "                continue\n",
    "        \n",
    "            try:\n",
    "                data = data_map[node]\n",
    "                schema = resolved_schema[f\"{node}.yaml\"]\n",
    "                validator = Draft4Validator(schema)\n",
    "                logging.info(f\"Validator set up for node {node}.\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error setting up validator for node {node}: {e}\")\n",
    "                continue\n",
    "\n",
    "            node_results = []\n",
    "            for idx, obj in enumerate(data):\n",
    "                try:\n",
    "                    result = self.validate_object(obj, idx, validator)\n",
    "                    result = {\"index_\" + str(idx): result}\n",
    "                    node_results.append(result)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error validating object at index {idx} for node {node}: {e}\")\n",
    "                \n",
    "            validation_results[node] = node_results\n",
    "        \n",
    "        return validation_results\n",
    "    \n",
    "    def list_entities(self) -> list:\n",
    "        \"\"\"\n",
    "        Lists all entities present in the validation results.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of entity names.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            entities = list(self.validation_result.keys())\n",
    "            logging.info(f\"Entities listed: {entities}\")\n",
    "            return entities\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error listing entities: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def list_index_by_entity(self, entity: str) -> list:\n",
    "        \"\"\"\n",
    "        Lists all index keys for a specified entity.\n",
    "\n",
    "        Args:\n",
    "            entity (str): The name of the entity to list index keys for.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of index keys for the specified entity.\n",
    "        \"\"\"\n",
    "        index_list = []\n",
    "        try:\n",
    "            for obj in self.validation_result[entity]:\n",
    "                index_list.append(list(obj.keys())[0])\n",
    "            logging.info(f\"Index keys for entity {entity}: {index_list}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error listing index by entity {entity}: {e}\")\n",
    "        return index_list\n",
    "    \n",
    "    def make_keymap(self) -> dict:\n",
    "        \"\"\"\n",
    "        Creates a dictionary that maps entities to their corresponding index keys.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary where each key is an entity name and each value is a list of index keys for that entity.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            entities = self.list_entities()\n",
    "            key_map = {}\n",
    "            for entity in entities:\n",
    "                key_map[entity] = self.list_index_by_entity(entity)\n",
    "            logging.info(f\"Keymap created: {key_map}\")\n",
    "            return key_map\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error making keymap: {e}\")\n",
    "            return {}\n",
    "        \n",
    "    def pull_entity(self, entity: str, result_type: str = \"FAIL\") -> list:\n",
    "        \"\"\"\n",
    "        Retrieves the validation results for a specified entity.\n",
    "\n",
    "        Args:\n",
    "            entity (str): The name of the entity to retrieve validation results for.\n",
    "            result_type (str, optional): The type of validation result to return. Either [\"PASS\", \"FAIL\", \"ALL\"]\n",
    "\n",
    "        Returns:\n",
    "            list: A list of validation results for the specified entity.\n",
    "        \"\"\"\n",
    "        return_objects = []\n",
    "        try:\n",
    "            for obj in self.validation_result[entity]:\n",
    "                obj_values = list(obj.values())\n",
    "                val_result = obj_values[0][0][\"validation_result\"]\n",
    "                \n",
    "                if result_type == \"ALL\":\n",
    "                    return_objects.append(obj)\n",
    "                    continue\n",
    "                \n",
    "                if val_result == result_type:\n",
    "                    return_objects.append(obj)\n",
    "            logging.info(f\"Pulled {result_type} results for entity {entity}: {len(return_objects)} results found.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error pulling entity {entity}: {e}\")\n",
    "\n",
    "        return return_objects\n",
    "\n",
    "    def pull_index_of_entity(self, entity: str, index_key: int, result_type: str = \"FAIL\", return_failed: bool = True) -> dict:\n",
    "        \"\"\"\n",
    "        Retrieves the validation result for a specified entity and index key.\n",
    "\n",
    "        Args:\n",
    "            entity (str): The name of the entity to retrieve validation results for.\n",
    "            index_key (int): The index key of the validation result to retrieve.\n",
    "            result_type (str, optional): The type of validation result to return. Either [\"PASS\", \"FAIL\", \"ALL\"]\n",
    "            return_failed (bool, optional): Flag to determine if only failed results should be returned.\n",
    "\n",
    "        Returns:\n",
    "            dict: The validation result for the specified entity and index key, or None if not found.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            data = self.validation_result[entity]\n",
    "            index_data = next((item[index_key] for item in data if index_key in item), None)\n",
    "            \n",
    "            return_list = []\n",
    "            for obj in index_data:\n",
    "                val_result = obj.get(\"validation_result\")\n",
    "                \n",
    "                if result_type == \"ALL\":\n",
    "                    return_list.append(obj)\n",
    "                    continue\n",
    "                \n",
    "                if val_result == result_type:\n",
    "                    return_list.append(obj)\n",
    "            \n",
    "            return return_list\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error pulling index of entity {entity} at index {index_key}: {e}\")\n",
    "            return []\n",
    "    \n",
    "\n",
    "\n",
    "class ValidateStats(Validate):\n",
    "    def __init__(self, validate_instance: Validate):\n",
    "        self.data_map = validate_instance.data_map\n",
    "        self.resolved_schema = validate_instance.resolved_schema\n",
    "        self.validation_result = validate_instance.validation_result\n",
    "        logging.info(\"Initializing ValidateStats class.\")\n",
    "        \n",
    "    \n",
    "    def n_rows_with_errors(self, entity: str) -> int:\n",
    "        \"\"\"\n",
    "        Returns the number of rows that have validation errors for a given entity.\n",
    "\n",
    "        Args:\n",
    "            entity (str): The name of the entity to check for validation errors.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of rows with validation errors.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            n_rows = len(self.pull_entity(entity))\n",
    "            logging.info(f\"Number of rows with errors for entity {entity}: {n_rows}\")\n",
    "            return n_rows\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error counting rows with errors for entity {entity}: {e}\")\n",
    "            return 0\n",
    "    \n",
    "    def count_results_by_index(self, entity: str, index_key: str, result_type: str = \"FAIL\", print_results: bool = False):\n",
    "        \"\"\"\n",
    "        Counts the number of validation results based on a specified entity and index_key.\n",
    "        For example the entity 'sample' will have an error in row 1 / index 1, which contains\n",
    "        5 validation errors due to errors in 5 columns for that row. So the method will return\n",
    "        5 validation errors.\n",
    "\n",
    "        Args:\n",
    "            entity (str): The name of the entity to count validation results for.\n",
    "            index_key (str): The key/index to count validation results for.\n",
    "            result_type (str, optional): The type of validation result to count. Either [\"PASS\", \"FAIL\", \"ALL\"]\n",
    "            print_results (bool, optional): Flag to print the results.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of validation results for the specified key/index.\n",
    "        \"\"\"\n",
    "        validation_count = 0\n",
    "        try:\n",
    "            index_data = self.pull_index_of_entity(entity = entity, index_key = index_key, result_type = result_type)\n",
    "            for obj in index_data:\n",
    "                val_result = obj[\"validation_result\"]\n",
    "                if result_type == \"ALL\":\n",
    "                    validation_count += 1\n",
    "                    continue\n",
    "\n",
    "            if val_result == result_type:\n",
    "                validation_count += 1\n",
    "\n",
    "            if print_results:\n",
    "                print(f\"Number of {result_type} validations for {entity} at {index_key}': {validation_count}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error counting results by index for entity {entity} at index {index_key}: {e}\")\n",
    "        return validation_count\n",
    "\n",
    "\n",
    "    def count_results_by_entity(self, entity: str, result_type: str = \"FAIL\", print_results: bool = False) -> int:\n",
    "        \"\"\"\n",
    "        Counts the number of validation results for a specified entity. Each entry in the \n",
    "        entity may produce more than one validation error, which will be counted. For \n",
    "        example, one entry, in 'sample' may result in 5 validation errors. This function counts\n",
    "        the total number of validation errors for a whole entity.\n",
    "\n",
    "        Args:\n",
    "            entity (str): The name of the entity to count failed validation results for.\n",
    "            result_type (str, optional): The type of validation result to count. Either [\"PASS\", \"FAIL\", \"ALL\"]\n",
    "            print_results (bool, optional): Flag to print the results.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of failed validation results for the specified entity.\n",
    "        \"\"\"\n",
    "        validation_count = 0\n",
    "        try:\n",
    "            index_keys = self.list_index_by_entity(entity=entity)\n",
    "            \n",
    "            for index_key in index_keys:\n",
    "                count = self.count_results_by_index(entity=entity, index_key=index_key, result_type=result_type)\n",
    "                validation_count += count\n",
    "            \n",
    "            if print_results:\n",
    "                logging.info(f\"Number of total {result_type} validations for '{entity}': {validation_count}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error counting results by entity {entity}: {e}\")\n",
    "        return validation_count\n",
    "    \n",
    "    def n_errors_per_entity(self, entity: str) -> int:\n",
    "        \"\"\"\n",
    "        Returns the number of errors that have validation errors for a given entity.\n",
    "\n",
    "        Args:\n",
    "            entity (str): The name of the entity to check for validation errors.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of rows with validation errors.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            n_errors = len(self.pull_entity(entity, return_failed=True))\n",
    "            logging.info(f\"Number of errors per entity {entity}: {n_errors}\")\n",
    "            return n_errors\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error counting errors per entity {entity}: {e}\")\n",
    "            return 0\n",
    "    \n",
    "    \n",
    "    def n_errors_per_entry(self, entity: str, index_key: int) -> int:\n",
    "        \"\"\"\n",
    "        Returns the number of validation errors for a given entity and index.\n",
    "\n",
    "        Args:\n",
    "            entity (str): The name of the entity to check for validation errors.\n",
    "            index_key (int): The index of the row to check for validation errors.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of validation errors for the given entity and index.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            n_errors = len(self.pull_index_of_entity(entity, index_key))\n",
    "            logging.info(f\"Number of errors per entry for entity {entity} at index {index_key}: {n_errors}\")\n",
    "            return n_errors\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error counting errors per entry for entity {entity} at index {index_key}: {e}\")\n",
    "            return 0\n",
    "    \n",
    "    def total_validation_errors(self) -> int:\n",
    "        \"\"\"\n",
    "        Calculates the total number of validation errors across all entities.\n",
    "\n",
    "        Returns:\n",
    "            int: The total number of validation errors.\n",
    "        \"\"\"\n",
    "        error_count = 0\n",
    "        try:\n",
    "            for entity in self.list_entities():\n",
    "                error_count += self.count_results_by_entity(entity=entity, result_type=\"FAIL\")\n",
    "            logging.info(f\"Total validation errors: {error_count}\")\n",
    "            print(f\"Total validation errors: {error_count}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error calculating total validation errors: {e}\")\n",
    "        return error_count\n",
    "    \n",
    "    \n",
    "    def summary_stats(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generates and prints a summary of validation statistics.\n",
    "\n",
    "        This method calculates the total number of validation errors across all entities\n",
    "        and provides detailed statistics for each entity, including the number of rows\n",
    "        with errors and the total number of errors per entity. The results are printed\n",
    "        to the console and returned as a pandas DataFrame.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame containing the summary statistics with columns\n",
    "            'entity', 'number_of_rows_with_errors', and 'number_of_errors_per_entity'.\n",
    "        \"\"\"\n",
    "        import pandas as pd\n",
    "\n",
    "        try:\n",
    "            total_errors = self.total_validation_errors()\n",
    "            logging.info(f\"Total validation errors: {total_errors}\")\n",
    "            \n",
    "            summary_data = []\n",
    "            for entity in self.list_entities():\n",
    "                n_rows_with_errors = self.n_rows_with_errors(entity)\n",
    "                n_errors_per_entity = self.count_results_by_entity(entity, result_type='FAIL')\n",
    "                \n",
    "                summary_data.append({\n",
    "                    \"entity\": entity,\n",
    "                    \"number_of_rows_with_errors\": n_rows_with_errors,\n",
    "                    \"number_of_errors_per_entity\": n_errors_per_entity\n",
    "                })\n",
    "            \n",
    "            summary_df = pd.DataFrame(summary_data)\n",
    "            logging.info(\"Summary statistics generated.\")\n",
    "            return summary_df\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error generating summary statistics: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "\n",
    "class ValidateSummary(Validate):\n",
    "    def __init__(self, validate_instance: Validate):\n",
    "        self.data_map = validate_instance.data_map\n",
    "        self.resolved_schema = validate_instance.resolved_schema\n",
    "        self.validation_result = validate_instance.validation_result\n",
    "        super().__init__(validate_instance.data_map, validate_instance.resolved_schema)\n",
    "        self.flattened_validation_results = None\n",
    "        logging.info(\"Initializing ValidateSummary class.\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    def flatten_validation_results(self, result_type: str = \"FAIL\") -> dict:\n",
    "        \"\"\"\n",
    "        Flattens the validation results created when initializing the Validate class.\n",
    "        \n",
    "        This method extracts all the validation results for each entity, each index row, \n",
    "        and each entry in the index row. It effectively pulls all the entries for a \n",
    "        particular entity, row, and column, where one row can produce validation errors \n",
    "        in multiple columns.\n",
    "\n",
    "        Args:\n",
    "            result_type (str): The type of validation result to filter by, default is \"FAIL\".\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing flattened validation results with a unique GUID \n",
    "            for each entry, along with the entity and other relevant validation details.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            key_map = self.make_keymap()\n",
    "            \n",
    "            flattened_results = []\n",
    "            for entity, index_list in key_map.items():\n",
    "                for index in index_list:\n",
    "                    index_obj = self.pull_index_of_entity(entity=entity, index_key=index, result_type=result_type)\n",
    "                    flattened_results.extend(\n",
    "                        {\"row\": index.strip(\"index_\"),\n",
    "                         \"entity\": entity,\n",
    "                         \"guid\": str(uuid.uuid4()), \n",
    "                         **obj} for obj in index_obj\n",
    "                    )\n",
    "            \n",
    "            self.flattened_validation_results = flattened_results\n",
    "            logging.info(f\"Flattened '{result_type}' validation results: {len(flattened_results)}\")\n",
    "            return flattened_results\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error flattening validation results: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def flattened_results_to_pd(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Transforms the flattened validation results into a pandas DataFrame.\n",
    "\n",
    "        This function retrieves the flattened validation results stored in the instance\n",
    "        and converts them into a pandas DataFrame. The DataFrame is then sorted by \n",
    "        'entity' and 'row' for organized analysis or processing.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame containing the sorted and indexed flattened \n",
    "            validation results.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(\"Converting flattened results to pandas dataframe...\")\n",
    "            pd_df = pd.json_normalize(self.flattened_validation_results)\n",
    "            pd_df.sort_values(by=['entity', 'row'], inplace=True)\n",
    "            pd_df.reset_index(drop=True, inplace=True)\n",
    "            return pd_df\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error converting flattened results to DataFrame: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def collapse_flatten_results_to_pd(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Collapses the flattened validation results into a summarized pandas DataFrame.\n",
    "\n",
    "        This method groups the flattened validation results by 'validation_error' and\n",
    "        aggregates other columns to provide a summary of the validation errors, including\n",
    "        the count of occurrences for each error type.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame containing the collapsed summary of validation errors,\n",
    "            sorted by entity, validation error, and count.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(\"Collapsing flattened results to pandas dataframe...\")\n",
    "            pd_df = pd.json_normalize(self.flattened_validation_results)\n",
    "            collapsed_df = pd_df.groupby('validation_error').agg({\n",
    "                'entity': 'first',\n",
    "                'row': 'count'\n",
    "            }).rename(columns={'row': 'count'}).reset_index()\n",
    "            collapsed_df = collapsed_df[['entity', 'count', 'validation_error']]\n",
    "            collapsed_df = collapsed_df.sort_values(by=['entity', 'validation_error', 'count']).reset_index(drop=True)\n",
    "            return collapsed_df\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error collapsing flattened results to DataFrame: {e}\")\n",
    "            return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the validation class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gen3_data_validator\n",
    "\n",
    "resolver = gen3_data_validator.ResolveSchema(schema_path = \"../schema/gen3_test_schema.json\")\n",
    "data = gen3_data_validator.ParseData(data_folder_path = \"../data/restricted/ausdiab_lipid_metadata/\")\n",
    "validator = gen3_data_validator.Validate(data_map=data.data_dict, resolved_schema=resolver.schema_resolved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting nested validation results\n",
    "- returns a nested dictionary by entity/data node then by the row/index number, and then the validation objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dict = validator.validation_result\n",
    "validation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator.list_entities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator.list_index_by_entity(\"lipidomics_assay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pull out a validation results for a specific entity with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator.pull_entity(\"lipidomics_assay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pull validation results for a specific entity and then a specific index / row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator.pull_index_of_entity(\"lipidomics_assay\", \"index_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting validation stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_stats = gen3_data_validator.ValidateStats(validator)\n",
    "stats_df = validate_stats.summary_stats()\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating validation summary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Summary = gen3_data_validator.ValidateSummary(validator) \n",
    "flattened_results_dict = Summary.flatten_validation_results()\n",
    "flattened_results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting flattened dict to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_summary_pd = Summary.flattened_results_to_pd()\n",
    "flatten_summary_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collapsing flattened dict to pandas\n",
    "- This collapsed data frame summarises common validation errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collapse_df = Summary.collapse_flatten_results_to_pd()\n",
    "collapse_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing validation results to folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "output_dir = \"../data/restricted/ausdiab_lipid_metadata/validation/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def write_dict_to_json(input_dict, output_dir, filename:str):\n",
    "    with open(f\"{output_dir}/{filename}.json\", \"w\") as f:\n",
    "        json.dump(input_dict, f)\n",
    "    print(f\"JSON files written to {output_dir}\")\n",
    "\n",
    "write_dict_to_json(validation_dict, output_dir, \"validation_dict\")\n",
    "write_dict_to_json(flattened_results_dict, output_dir, \"flattened_results_dict\")\n",
    "\n",
    "# Writing pandas df\n",
    "stats_df.to_csv(f\"{output_dir}/stats_df.csv\")\n",
    "flatten_summary_pd.to_csv(f\"{output_dir}/flatten_summary_pd.csv\")\n",
    "collapse_df.to_csv(f\"{output_dir}/collapse_df.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this for writing tests\n",
    "\n",
    "sample_validation_results = {\n",
    "    'sample': [\n",
    "        [\n",
    "            {\n",
    "                'index': 0,\n",
    "                'validation_result': 'FAIL',\n",
    "                'invalid_key': 'freeze_thaw_cycles',\n",
    "                'schema_path': 'properties.freeze_thaw_cycles.type',\n",
    "                'validator': 'type',\n",
    "                'validator_value': 'integer',\n",
    "                'validation_error': \"'10' is not of type 'integer'\"\n",
    "            },\n",
    "            {\n",
    "                'index': 0,\n",
    "                'validation_result': 'FAIL',\n",
    "                'invalid_key': 'sample_provider',\n",
    "                'schema_path': 'properties.sample_provider.enum',\n",
    "                'validator': 'enum',\n",
    "                'validator_value': ['Baker', 'USYD', 'UMELB', 'UQ'],\n",
    "                'validation_error': \"45 is not one of ['Baker', 'USYD', 'UMELB', 'UQ']\"\n",
    "            },\n",
    "            {\n",
    "                'index': 0,\n",
    "                'validation_result': 'FAIL',\n",
    "                'invalid_key': 'sample_storage_method',\n",
    "                'schema_path': 'properties.sample_storage_method.enum',\n",
    "                'validator': 'enum',\n",
    "                'validator_value': [\n",
    "                    'not stored',\n",
    "                    'ambient temperature',\n",
    "                    'cut slide',\n",
    "                    'fresh',\n",
    "                    'frozen, -70C freezer',\n",
    "                    'frozen, -150C freezer',\n",
    "                    'frozen, liquid nitrogen',\n",
    "                    'frozen, vapor phase',\n",
    "                    'paraffin block',\n",
    "                    'RNAlater, frozen',\n",
    "                    'TRIzol, frozen'\n",
    "                ],\n",
    "                'validation_error': \"'Autoclave' is not one of ['not stored', 'ambient temperature', 'cut slide', 'fresh', 'frozen, -70C freezer', 'frozen, -150C freezer', 'frozen, liquid nitrogen', 'frozen, vapor phase', 'paraffin block', 'RNAlater, frozen', 'TRIzol, frozen']\"\n",
    "            }\n",
    "        ],\n",
    "        [\n",
    "            {\n",
    "                'index': 1,\n",
    "                'validation_result': 'FAIL',\n",
    "                'invalid_key': 'freeze_thaw_cycles',\n",
    "                'schema_path': 'properties.freeze_thaw_cycles.type',\n",
    "                'validator': 'type',\n",
    "                'validator_value': 'integer',\n",
    "                'validation_error': \"'76' is not of type 'integer'\"\n",
    "            },\n",
    "            {\n",
    "                'index': 1,\n",
    "                'validation_result': 'FAIL',\n",
    "                'invalid_key': 'sample_storage_method',\n",
    "                'schema_path': 'properties.sample_storage_method.enum',\n",
    "                'validator': 'enum',\n",
    "                'validator_value': [\n",
    "                    'not stored',\n",
    "                    'ambient temperature',\n",
    "                    'cut slide',\n",
    "                    'fresh',\n",
    "                    'frozen, -70C freezer',\n",
    "                    'frozen, -150C freezer',\n",
    "                    'frozen, liquid nitrogen',\n",
    "                    'frozen, vapor phase',\n",
    "                    'paraffin block',\n",
    "                    'RNAlater, frozen',\n",
    "                    'TRIzol, frozen'\n",
    "                ],\n",
    "                'validation_error': \"'In the Pantry' is not one of ['not stored', 'ambient temperature', 'cut slide', 'fresh', 'frozen, -70C freezer', 'frozen, -150C freezer', 'frozen, liquid nitrogen', 'frozen, vapor phase', 'paraffin block', 'RNAlater, frozen', 'TRIzol, frozen']\"\n",
    "            }\n",
    "        ],\n",
    "        [\n",
    "            {\n",
    "                'index': 2,\n",
    "                'validation_result': 'PASS',\n",
    "                'invalid_key': None,\n",
    "                'schema_path': None,\n",
    "                'validator': None,\n",
    "                'validator_value': None,\n",
    "                'validation_error': None\n",
    "            }\n",
    "        ],\n",
    "        [\n",
    "            {\n",
    "                'index': 3,\n",
    "                'validation_result': 'PASS',\n",
    "                'invalid_key': None,\n",
    "                'schema_path': None,\n",
    "                'validator': None,\n",
    "                'validator_value': None,\n",
    "                'validation_error': None\n",
    "            }\n",
    "        ]\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
