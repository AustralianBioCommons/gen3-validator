{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolve Schema\n",
    "\n",
    "**input**\n",
    "```\n",
    "+schema_path: str\n",
    "```\n",
    "\n",
    "**methods**\n",
    "```\n",
    "+read_json(schema_path: str): dict\n",
    "+split_json(schema: dict): list\n",
    "+resolver(entity1: dict, entity2: dict): dict\n",
    "+resolve_defs(terms: dict, defs: dict) : dict\n",
    "+ node_order(schema: dict): list\n",
    "+resolve_nodes(nodeList: list, splitJsonList: list): list\n",
    "+recombine_nodes(resolvedList: list) : dict\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict, deque\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "class ResolveSchema:\n",
    "    \n",
    "    def __init__(self, schema_path: str):\n",
    "        \"\"\"\n",
    "        Initialize the ResolveSchema class.\n",
    "\n",
    "        Parameters:\n",
    "        - schema_path (str): The path to the JSON schema file.\n",
    "        \"\"\"\n",
    "        self.schema_path = schema_path\n",
    "        self.schema = self.read_json(self.schema_path)\n",
    "        self.nodes = self.get_nodes()\n",
    "        self.node_pairs = self.get_all_node_pairs()\n",
    "        self.node_order = self.get_node_order(edges=self.node_pairs)\n",
    "        self.schema_list = self.split_json()\n",
    "        self.schema_def = self.return_schema(\"_definitions.yaml\")\n",
    "        self.schema_term = self.return_schema(\"_terms.yaml\")\n",
    "        self.schema_def_resolved = self.resolve_references(self.schema_def, self.schema_term)\n",
    "        self.schema_list_resolved = self.resolve_all_references()\n",
    "        \n",
    "    def read_json(self, path: str) -> dict:\n",
    "        \"\"\"\n",
    "        Read a JSON file and return its contents as a dictionary.\n",
    "\n",
    "        Parameters:\n",
    "        - path (str): The path to the JSON file.\n",
    "\n",
    "        Returns:\n",
    "        - dict: The contents of the JSON file.\n",
    "        \"\"\"\n",
    "        with open(path) as f:\n",
    "            return json.load(f)\n",
    "    \n",
    "    def get_nodes(self) -> list:\n",
    "        \"\"\"\n",
    "        Retrieve all node names from the schema.\n",
    "\n",
    "        Returns:\n",
    "        - list: A list of node names.\n",
    "        \"\"\"\n",
    "        nodes = list(self.schema.keys())\n",
    "        return nodes\n",
    "    \n",
    "    def get_node_link(self, node_name: str) -> tuple:\n",
    "        \"\"\"\n",
    "        Retrieve the links and ID for a given node.\n",
    "\n",
    "        Parameters:\n",
    "        - node_name (str): The name of the node.\n",
    "\n",
    "        Returns:\n",
    "        - tuple: A tuple containing the node ID and its links.\n",
    "        \"\"\"\n",
    "        links = self.schema[node_name]['links']\n",
    "        node_id = self.schema[node_name]['id']\n",
    "        if 'subgroup' in links[0]:\n",
    "            return node_id, links[0]['subgroup']\n",
    "        else:\n",
    "            return node_id, links\n",
    "\n",
    "    def find_upstream_downstream(self, node_name: str) -> list:\n",
    "        \"\"\"\n",
    "        Takes a node name and returns the upstream and downstream nodes.\n",
    "\n",
    "        Parameters:\n",
    "        - node_name (str): The name of the node.\n",
    "\n",
    "        Returns:\n",
    "        - list: A list of tuples representing upstream and downstream nodes.\n",
    "        \"\"\"\n",
    "        node_id, links = self.get_node_link(node_name)\n",
    "        \n",
    "        # Ensure links is a list\n",
    "        if isinstance(links, dict):\n",
    "            links = [links]\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for link in links:\n",
    "            target_type = link.get(\"target_type\")\n",
    "            \n",
    "            if not node_id or not target_type:\n",
    "                print(\"Missing essential keys in link:\", link)\n",
    "                results.append((None, None))\n",
    "                continue\n",
    "\n",
    "            results.append((target_type, node_id))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def get_all_node_pairs(self, excluded_nodes=[\"_definitions.yaml\", \"_terms.yaml\", \"_settings.yaml\", \"program.yaml\"]) -> list:\n",
    "        \"\"\"\n",
    "        Retrieve all node pairs, excluding specified nodes.\n",
    "\n",
    "        Parameters:\n",
    "        - excluded_nodes (list): A list of node names to exclude.\n",
    "\n",
    "        Returns:\n",
    "        - list: A list of node pairs.\n",
    "        \"\"\"\n",
    "        node_pairs = []\n",
    "        for node in self.nodes:\n",
    "            if not node in excluded_nodes:\n",
    "                node_pairs.extend(self.find_upstream_downstream(node))\n",
    "            else:\n",
    "                continue\n",
    "        return node_pairs\n",
    "    \n",
    "    def get_node_order(self, edges: list) -> list:\n",
    "        \"\"\"\n",
    "        Determine the order of nodes based on their dependencies.\n",
    "\n",
    "        Parameters:\n",
    "        - edges (list): A list of tuples representing node dependencies.\n",
    "\n",
    "        Returns:\n",
    "        - list: A list of nodes in topological order.\n",
    "        \"\"\"\n",
    "        # Build graph representation\n",
    "        graph = defaultdict(list)\n",
    "        in_degree = defaultdict(int)\n",
    "\n",
    "        for upstream, downstream in edges:\n",
    "            graph[upstream].append(downstream)\n",
    "            in_degree[downstream] += 1\n",
    "            if upstream not in in_degree:\n",
    "                in_degree[upstream] = 0\n",
    "\n",
    "        # Perform Topological Sorting (Kahn's Algorithm)\n",
    "        sorted_order = []\n",
    "        zero_in_degree = deque([node for node in in_degree if in_degree[node] == 0])\n",
    "\n",
    "        while zero_in_degree:\n",
    "            node = zero_in_degree.popleft()\n",
    "            sorted_order.append(node)\n",
    "            \n",
    "            for neighbor in graph[node]:\n",
    "                in_degree[neighbor] -= 1\n",
    "                if in_degree[neighbor] == 0:\n",
    "                    zero_in_degree.append(neighbor)\n",
    "\n",
    "        # Ensure core_metadata_collection is last\n",
    "        sorted_order.remove(\"core_metadata_collection\")\n",
    "        sorted_order.append(\"core_metadata_collection\")\n",
    "\n",
    "        return sorted_order\n",
    "    \n",
    "    def split_json(self) -> list:\n",
    "        \"\"\"\n",
    "        Split the schema into a list of individual node schemas.\n",
    "\n",
    "        Returns:\n",
    "        - list: A list of node schemas.\n",
    "        \"\"\"\n",
    "        schema_list = []\n",
    "        for node in self.nodes:\n",
    "            schema_list.append(self.schema[node])\n",
    "        return schema_list\n",
    "    \n",
    "    def return_schema(self, target_id: str) -> dict:\n",
    "        \"\"\"\n",
    "        Retrieves the first dictionary from a list where the 'id' key matches the target_id.\n",
    "\n",
    "        Parameters:\n",
    "        - target_id (str): The value of the 'id' key to match.\n",
    "\n",
    "        Returns:\n",
    "        - dict: The dictionary that matches the target_id, or None if not found.\n",
    "        \"\"\"\n",
    "        if target_id.endswith('.yaml'):\n",
    "            target_id = target_id[:-5]\n",
    "        \n",
    "        result = next((item for item in self.schema_list if item.get('id') == target_id), None)\n",
    "        if result is None:\n",
    "            print(f\"{target_id} not found\")\n",
    "        return result\n",
    "    \n",
    "    def resolve_references(self, schema: dict, reference: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Takes a gen3 jsonschema draft 4 as a dictionary and recursively resolves any references using a reference schema which has no references.\n",
    "\n",
    "        Parameters:\n",
    "        - schema (dict): The JSON node to resolve references in.\n",
    "        - reference (dict): The schema containing the references.\n",
    "\n",
    "        Returns:\n",
    "        - dict: The resolved JSON node with references resolved.\n",
    "        \"\"\"\n",
    "        ref_input_content = reference\n",
    "\n",
    "        def resolve_node(node, manual_ref_content=ref_input_content):\n",
    "            if isinstance(node, dict):\n",
    "                if '$ref' in node:\n",
    "                    ref_path = node['$ref']\n",
    "                    ref_file, ref_key = ref_path.split('#')\n",
    "                    ref_file = ref_file.strip()\n",
    "                    ref_key = ref_key.strip('/')\n",
    "                \n",
    "                    # if a reference file is in the reference, load the pre-defined reference, if no file exists, then use the schema itself as reference\n",
    "                    if ref_file:\n",
    "                        ref_content = manual_ref_content\n",
    "                    else:\n",
    "                        ref_content = schema\n",
    "                    \n",
    "                    for part in ref_key.split('/'):\n",
    "                        ref_content = ref_content[part]\n",
    "\n",
    "                    resolved_content = resolve_node(ref_content)\n",
    "                    # Merge resolved content with the current node, excluding the $ref key\n",
    "                    return {**resolved_content, **{k: resolve_node(v) for k, v in node.items() if k != '$ref'}}\n",
    "                else:\n",
    "                    return {k: resolve_node(v) for k, v in node.items()}\n",
    "            elif isinstance(node, list):\n",
    "                return [resolve_node(item) for item in node]\n",
    "            else:\n",
    "                return node\n",
    "\n",
    "        return resolve_node(schema)\n",
    "    \n",
    "    def resolve_all_references(self) -> list:\n",
    "        \"\"\"\n",
    "        Resolves references in all other schema dictionaries using the resolved definitions schema.\n",
    "\n",
    "        Returns:\n",
    "        - list: A list of resolved schema dictionaries.\n",
    "        \"\"\"\n",
    "        resolved_schema_list = []\n",
    "        for node in self.nodes:\n",
    "            \n",
    "            if node == \"_definitions.yaml\" or node == \"_terms.yaml\":\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                resolved_schema = self.resolve_references(self.schema[node], self.schema_def_resolved)\n",
    "                resolved_schema_list.append(resolved_schema)\n",
    "                print(f\"Resolved {node}\")\n",
    "            except KeyError as e:\n",
    "                print(f\"Error resolving {node}: Missing key {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error resolving {node}: {e}\")\n",
    "                \n",
    "        return resolved_schema_list\n",
    "    \n",
    "    \n",
    "    def return_resolved_schema(self, target_id: str) -> dict:\n",
    "        \"\"\"\n",
    "        Retrieves the first dictionary from a list where the 'id' key matches the target_id.\n",
    "\n",
    "        Parameters:\n",
    "        - target_id (str): The value of the 'id' key to match.\n",
    "\n",
    "        Returns:\n",
    "        - dict: The dictionary that matches the target_id, or None if not found.\n",
    "        \"\"\"\n",
    "        if target_id.endswith('.yaml'):\n",
    "            target_id = target_id[:-5]\n",
    "        \n",
    "        result = next((item for item in self.schema_list_resolved if item.get('id') == target_id), None)\n",
    "        if result is None:\n",
    "            print(f\"{target_id} not found\")\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    \n",
    "#### Parse Data Class\n",
    "\n",
    "class ParseData():\n",
    "    \"\"\"Parses a json data into a list of dictionaries\"\"\"\n",
    "    \n",
    "    def __init__(self, data_folder_path: str = None, data_file_path: str = None):\n",
    "        self.folder_path = data_folder_path\n",
    "        self.file_path = data_file_path\n",
    "        self.file_path_list = self.list_data_files()\n",
    "        self.data_list = self.load_json_data(self.file_path_list)\n",
    "        self.data_nodes = self.get_node_names()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def read_json(self, path: str) -> dict:\n",
    "        with open(path) as f:\n",
    "            data = json.load(f)\n",
    "            return data\n",
    "    \n",
    "    \n",
    "    def list_data_files(self) -> list:\n",
    "        \"\"\"\n",
    "        Lists all JSON data files in the specified folder or returns the single file path.\n",
    "\n",
    "        This method checks if a folder path is provided. If so, it lists all files in the folder\n",
    "        that have a '.json' extension and returns their absolute paths. If no folder path is provided,\n",
    "        it returns the single file path specified during initialization.\n",
    "\n",
    "        Returns:\n",
    "        - list: A list of absolute file paths to JSON files.\n",
    "        \"\"\"\n",
    "        if self.folder_path:\n",
    "            json_paths = [os.path.abspath(os.path.join(self.folder_path, f)) for f in os.listdir(self.folder_path) if f.endswith(\".json\")]\n",
    "        else:\n",
    "            json_paths = [self.file_path]\n",
    "        return json_paths\n",
    "        \n",
    "        \n",
    "    def load_json_data(self, json_paths: list) -> list:\n",
    "        \"\"\"\n",
    "        Loads JSON data from a list of file paths.\n",
    "\n",
    "        This method reads each JSON file specified in the json_paths list,\n",
    "        loads the data, and appends it to a list of JSON objects.\n",
    "\n",
    "        Parameters:\n",
    "        - json_paths (list): A list of file paths to JSON files.\n",
    "\n",
    "        Returns:\n",
    "        - list: A list of dictionaries containing the data from each JSON file.\n",
    "        \"\"\"\n",
    "        json_files = []\n",
    "        for file in json_paths:\n",
    "            json_data = self.read_json(file)\n",
    "            json_files.append(json_data)\n",
    "            print(f\"Loaded {file}\")\n",
    "        \n",
    "        return json_files\n",
    "    \n",
    "    \n",
    "    def get_node_names(self) -> list:\n",
    "        \"\"\"\n",
    "        Retrieves the names of nodes from the JSON files.\n",
    "\n",
    "        This method iterates over the list of file paths and extracts the node names\n",
    "        by removing the '.json' extension from each file name.\n",
    "\n",
    "        Returns:\n",
    "        - list: A list of node names extracted from the JSON file paths.\n",
    "        \"\"\"\n",
    "        node_names = []\n",
    "        for node in self.file_path_list:\n",
    "            if node.endswith('.json'):\n",
    "                last_item = os.path.basename(node)\n",
    "                node_names.append(last_item[:-5])\n",
    "            else:\n",
    "                node_names.append(node)\n",
    "        return node_names\n",
    "\n",
    "\n",
    "class ParseXlsxMetadata():\n",
    "    def __init__(self, xlsx_path: str):\n",
    "        self.xlsx_path = xlsx_path\n",
    "        self.xlsx_data_dict = self.parse_metadata_template()\n",
    "        self.sheet_names = self.get_sheet_names()\n",
    "        \n",
    "    def parse_metadata_template(self) -> dict:\n",
    "        \"\"\"\n",
    "        Parses an Excel file and converts each sheet into a DataFrame.\n",
    "\n",
    "        This function reads an Excel file specified by the `xlsx_path` and loads each sheet\n",
    "        into a dictionary where the keys are the sheet names and the values are the DataFrames\n",
    "        representing the data in those sheets. The first row of each DataFrame is removed.\n",
    "\n",
    "        Args:\n",
    "        - xlsx_path (str): The path to the Excel file to be parsed.\n",
    "\n",
    "        Returns:\n",
    "        - dict: A dictionary where each key is a sheet name and each value is a DataFrame\n",
    "        containing the data from that sheet, with the first row removed.\n",
    "        \"\"\"\n",
    "        # load xlsx file\n",
    "        pd_dict = pd.read_excel(self.xlsx_path, sheet_name=None)\n",
    "\n",
    "        # in each pandas data fram in the dict, remove the first row\n",
    "        for key in pd_dict.keys():\n",
    "            pd_dict[key] = pd_dict[key].iloc[1:, :]\n",
    "\n",
    "        return pd_dict\n",
    "    \n",
    "    \n",
    "    def get_sheet_names(self) -> list:\n",
    "        return list(self.xlsx_data_dict.keys())\n",
    "    \n",
    "    \n",
    "    def get_pk_fk_pairs(self, sheet_name: str) -> tuple:\n",
    "        \"\"\"\n",
    "        Extracts the primary key (PK) and foreign key (FK) column names from a specified sheet.\n",
    "\n",
    "        This method retrieves the first two column names from the given sheet in the Excel data dictionary,\n",
    "        assuming the first column is the primary key and the second column is the foreign key.\n",
    "\n",
    "        Args:\n",
    "        - sheet_name (str): The name of the sheet from which to extract the PK and FK.\n",
    "\n",
    "        Returns:\n",
    "        - tuple: A tuple containing the primary key and foreign key column names.\n",
    "        \"\"\"\n",
    "        sheet = self.xlsx_data_dict[sheet_name]\n",
    "        first_two_columns = sheet.columns[:2].tolist()\n",
    "        pk = first_two_columns[0]\n",
    "        fk = first_two_columns[1]\n",
    "        return pk, fk\n",
    "    \n",
    "    \n",
    "    def pd_to_json(self, sheet_name: str, json_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Converts a specified sheet from the metadata dictionary to a JSON file. \n",
    "        Also formats and renames the primary and foreign keys into a gen3 compatible format\n",
    "\n",
    "        Args:\n",
    "        - metadata (dict): A dictionary where each key is a sheet name and each value is a DataFrame.\n",
    "        - sheet_name (str): The name of the sheet to convert to JSON.\n",
    "        - json_path (str): The path to the JSON file to be saved.\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        \n",
    "        pk, fk = self.get_pk_fk_pairs(sheet_name)\n",
    "\n",
    "        df = self.xlsx_data_dict[sheet_name]\n",
    "        df['type'] = sheet_name # add node / entity name\n",
    "        fk_name = fk.split('_uid')[0]\n",
    "        df[f\"{fk_name}s\"] = df[fk].apply(lambda x: {\"submitter_id\": x}) # format foreign key\n",
    "        df_cleaned = df.where(pd.notnull(df), None)\n",
    "        df_cleaned['submitter_id'] = df_cleaned[pk].tolist() # adding primary key to submitter_id key\n",
    "        df_cleaned = df_cleaned.loc[:, ~df_cleaned.columns.str.endswith('_uid')]\n",
    "        data_list = df_cleaned.to_dict(orient='records')\n",
    "        \n",
    "\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(data_list, f)\n",
    "\n",
    "        \n",
    "class TestLinkage(ResolveSchema, ParseData):\n",
    "    def __init__(self, schema_path, data_folder_path: str = None, data_file_path: str = None):\n",
    "        super().__init__(schema_path)\n",
    "        self.data_inst = ParseData(data_folder_path, data_file_path)\n",
    "        self.data_list = self.data_inst.data_list\n",
    "        self.data_nodes = self.data_inst.data_nodes\n",
    "        \n",
    "    # def pull_keys(self, suffix: str = '_uid'):\n",
    "        \n",
    "    \n",
    "    \n",
    "    # def find_pk(self, suffix: str = '_uid')\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_inst = ParseXlsxMetadata('../data/lipid_metadata_example.xlsx')\n",
    "metadata_sheets = metadata_inst.sheet_names\n",
    "\n",
    "for sheet in metadata_sheets:\n",
    "    metadata_inst.pd_to_json(sheet, f'../data/lipid_pass/{sheet}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and saving xlsx to individual jsons based on tabs\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def parse_metadata_template(xlsx_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Parses an Excel file and converts each sheet into a DataFrame.\n",
    "\n",
    "    This function reads an Excel file specified by the `xlsx_path` and loads each sheet\n",
    "    into a dictionary where the keys are the sheet names and the values are the DataFrames\n",
    "    representing the data in those sheets. The first row of each DataFrame is removed.\n",
    "\n",
    "    Args:\n",
    "    - xlsx_path (str): The path to the Excel file to be parsed.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary where each key is a sheet name and each value is a DataFrame\n",
    "      containing the data from that sheet, with the first row removed.\n",
    "    \"\"\"\n",
    "    # load xlsx file\n",
    "    pd_dict = pd.read_excel(xlsx_path, sheet_name=None)\n",
    "\n",
    "    # in each pandas data fram in the dict, remove the first row\n",
    "    for key in pd_dict.keys():\n",
    "        pd_dict[key] = pd_dict[key].iloc[1:, :]\n",
    "\n",
    "    return pd_dict\n",
    "\n",
    "metadata = parse_metadata_template('../data/lipid_metadata.xlsx')\n",
    "\n",
    "\n",
    "def xlsx_to_json(xlsx_path: str, json_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Converts an Excel file to a JSON file.\n",
    "\n",
    "    This function reads an Excel file specified by the `xlsx_path` and loads each sheet\n",
    "    into a dictionary where the keys are the sheet names and the values are the DataFrames\n",
    "    representing the data in those sheets. The first row of each DataFrame is removed.\n",
    "\n",
    "    Args:\n",
    "    - xlsx_path (str): The path to the Excel file to be parsed.\n",
    "    - json_path (str): The path to the JSON file to be saved.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    pd_dict = parse_metadata_template(xlsx_path)\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(pd_dict, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def pd_to_json(metadata: dict, sheet_name: str, json_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Converts a specified sheet from the metadata dictionary to a JSON file.\n",
    "\n",
    "    Args:\n",
    "    - metadata (dict): A dictionary where each key is a sheet name and each value is a DataFrame.\n",
    "    - sheet_name (str): The name of the sheet to convert to JSON.\n",
    "    - json_path (str): The path to the JSON file to be saved.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    data_list = metadata[sheet_name].reset_index().rename(columns={'index': 'id'}).to_dict(orient='records')\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(data_list, f)\n",
    "\n",
    "# Example usage\n",
    "pd_to_json(metadata, 'lipidomics_mapping_file', '../data/lipidomics_mapping_file.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved demographic.yaml\n",
      "Resolved project.yaml\n",
      "Resolved serum_marker_assay.yaml\n",
      "Resolved alignment_workflow.yaml\n",
      "Resolved imaging_file.yaml\n",
      "Resolved lipidomics_assay.yaml\n",
      "Resolved metabolomics_file.yaml\n",
      "Resolved acknowledgement.yaml\n",
      "Resolved medical_history.yaml\n",
      "Resolved _settings.yaml\n",
      "Resolved blood_pressure_test.yaml\n",
      "Resolved genomics_assay.yaml\n",
      "Resolved variant_file.yaml\n",
      "Resolved program.yaml\n",
      "Resolved serum_marker_file.yaml\n",
      "Resolved proteomics_assay.yaml\n",
      "Resolved sample.yaml\n",
      "Resolved unaligned_reads_file.yaml\n",
      "Resolved aligned_reads_index_file.yaml\n",
      "Resolved variant_workflow.yaml\n",
      "Resolved proteomics_file.yaml\n",
      "Resolved exposure.yaml\n",
      "Resolved metabolomics_assay.yaml\n",
      "Resolved lipidomics_mapping_file.yaml\n",
      "Resolved lipidomics_file.yaml\n",
      "Resolved aligned_reads_file.yaml\n",
      "Resolved lab_result.yaml\n",
      "Resolved medication.yaml\n",
      "Resolved publication.yaml\n",
      "Resolved subject.yaml\n",
      "Resolved core_metadata_collection.yaml\n",
      "Loaded /Users/harrijh/projects/gen3-data-validator/data/pass/metabolomics_file.json\n",
      "Loaded /Users/harrijh/projects/gen3-data-validator/data/pass/medical_history.json\n",
      "Loaded /Users/harrijh/projects/gen3-data-validator/data/pass/metabolomics_assay.json\n",
      "Loaded /Users/harrijh/projects/gen3-data-validator/data/pass/sample.json\n",
      "Loaded /Users/harrijh/projects/gen3-data-validator/data/pass/subject.json\n"
     ]
    }
   ],
   "source": [
    "linkage_class = TestLinkage(schema_path = \"../schema/gen3_test_schema.json\", data_folder_path = \"../data/pass\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['metabolomics_file',\n",
       " 'medical_history',\n",
       " 'metabolomics_assay',\n",
       " 'sample',\n",
       " 'subject']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded /Users/harrijh/projects/gen3-data-validator/data/pass/metabolomics_file.json\n",
      "Loaded /Users/harrijh/projects/gen3-data-validator/data/pass/medical_history.json\n",
      "Loaded /Users/harrijh/projects/gen3-data-validator/data/pass/metabolomics_assay.json\n",
      "Loaded /Users/harrijh/projects/gen3-data-validator/data/pass/sample.json\n",
      "Loaded /Users/harrijh/projects/gen3-data-validator/data/pass/subject.json\n"
     ]
    }
   ],
   "source": [
    "data = ParseData(data_folder_path = \"../data/pass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['metabolomics_file',\n",
       " 'medical_history',\n",
       " 'metabolomics_assay',\n",
       " 'sample',\n",
       " 'subject']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.get_node_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved demographic.yaml\n",
      "Resolved project.yaml\n",
      "Resolved serum_marker_assay.yaml\n",
      "Resolved alignment_workflow.yaml\n",
      "Resolved imaging_file.yaml\n",
      "Resolved lipidomics_assay.yaml\n",
      "Resolved metabolomics_file.yaml\n",
      "Resolved acknowledgement.yaml\n",
      "Resolved medical_history.yaml\n",
      "Resolved _settings.yaml\n",
      "Resolved blood_pressure_test.yaml\n",
      "Resolved genomics_assay.yaml\n",
      "Resolved variant_file.yaml\n",
      "Resolved program.yaml\n",
      "Resolved serum_marker_file.yaml\n",
      "Resolved proteomics_assay.yaml\n",
      "Resolved sample.yaml\n",
      "Resolved unaligned_reads_file.yaml\n",
      "Resolved aligned_reads_index_file.yaml\n",
      "Resolved variant_workflow.yaml\n",
      "Resolved proteomics_file.yaml\n",
      "Resolved exposure.yaml\n",
      "Resolved metabolomics_assay.yaml\n",
      "Resolved lipidomics_mapping_file.yaml\n",
      "Resolved lipidomics_file.yaml\n",
      "Resolved aligned_reads_file.yaml\n",
      "Resolved lab_result.yaml\n",
      "Resolved medication.yaml\n",
      "Resolved publication.yaml\n",
      "Resolved subject.yaml\n",
      "Resolved core_metadata_collection.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'$schema': 'http://json-schema.org/draft-04/schema#',\n",
       " 'additionalProperties': False,\n",
       " 'category': 'clinical',\n",
       " 'description': 'An individual participant in the study with baseline measurements.',\n",
       " 'id': 'subject',\n",
       " 'links': [{'backref': 'subjects',\n",
       "   'label': 'part_of',\n",
       "   'multiplicity': 'many_to_one',\n",
       "   'name': 'projects',\n",
       "   'required': True,\n",
       "   'target_type': 'project'}],\n",
       " 'namespace': 'https://data.test.biocommons.org.au/',\n",
       " 'program': '*',\n",
       " 'project': '*',\n",
       " 'properties': {'created_datetime': {'oneOf': [{'format': 'date-time',\n",
       "     'type': 'string'},\n",
       "    {'type': 'null'}],\n",
       "   'term': {'description': 'A combination of date and time of day in the form [-]CCYY-MM-DDThh:mm:ss[Z|(+|-)hh:mm]\\n'}},\n",
       "  'id': {'pattern': '^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$',\n",
       "   'term': {'description': 'A 128-bit identifier. Depending on the mechanism used to generate it, it is either guaranteed to be different from all other UUIDs/GUIDs generated until 3400 AD or extremely likely to be different. Its relatively small size lends itself well to sorting, ordering, and hashing of all sorts, storing in databases, simple allocation, and ease of programming in general.\\n',\n",
       "    'termDef': {'cde_id': 'C54100',\n",
       "     'cde_version': None,\n",
       "     'source': 'NCIt',\n",
       "     'term': 'Universally Unique Identifier',\n",
       "     'term_url': 'https://ncit.nci.nih.gov/ncitbrowser/ConceptReport.jsp?dictionary=NCI_Thesaurus&version=16.02d&ns=NCI_Thesaurus&code=C54100'}},\n",
       "   'type': 'string',\n",
       "   'systemAlias': 'node_id'},\n",
       "  'project_id': {'term': {'description': 'Unique ID for any specific defined piece of work that is undertaken or attempted to meet a single requirement.\\n'},\n",
       "   'type': 'string'},\n",
       "  'state': {'default': 'validated',\n",
       "   'downloadable': ['uploaded',\n",
       "    'md5summed',\n",
       "    'validating',\n",
       "    'validated',\n",
       "    'error',\n",
       "    'invalid',\n",
       "    'released'],\n",
       "   'oneOf': [{'enum': ['uploading',\n",
       "      'uploaded',\n",
       "      'md5summing',\n",
       "      'md5summed',\n",
       "      'validating',\n",
       "      'error',\n",
       "      'invalid',\n",
       "      'suppressed',\n",
       "      'redacted',\n",
       "      'live']},\n",
       "    {'enum': ['validated', 'submitted', 'released']}],\n",
       "   'public': ['live'],\n",
       "   'term': {'description': 'The current state of the object.\\n'}},\n",
       "  'submitter_id': {'description': 'A project-specific identifier for a node. This property is the calling card/nickname/alias for a unit of submission. It can be used in place of the UUID for identifying or recalling a node.\\n',\n",
       "   'type': ['string']},\n",
       "  'type': {'type': 'string'},\n",
       "  'updated_datetime': {'oneOf': [{'format': 'date-time', 'type': 'string'},\n",
       "    {'type': 'null'}],\n",
       "   'term': {'description': 'A combination of date and time of day in the form [-]CCYY-MM-DDThh:mm:ss[Z|(+|-)hh:mm]\\n'}},\n",
       "  'cohort_id': {'description': 'Identifier for the cohort the subject is a part of',\n",
       "   'type': 'string'},\n",
       "  'consent_codes': {'description': 'Subject Consent | Data Use Restrictions that are used to indicate  permissions/restrictions for datasets and/or materials, and relates to the purposes for which datasets and/or material might be removed, stored or used. Based on the Data Use Ontology : see http://www.obofoundry.org/ontology/duo.html',\n",
       "   'items': {'enum': ['General Research Use',\n",
       "     'Health or Biomedical Research',\n",
       "     'Disease Specific Research',\n",
       "     'not for profit, non commercial user only',\n",
       "     'ethics approval required',\n",
       "     'user specific restriction'],\n",
       "    'enumDef': [{'enumeration': 'General Research Use',\n",
       "      'source': 'duo',\n",
       "      'term_id': 'DUO:0000042'},\n",
       "     {'enumeration': 'Health or Biomedical Research',\n",
       "      'source': 'duo',\n",
       "      'term_id': 'DUO:0000006'},\n",
       "     {'enumeration': 'Disease Specific Research',\n",
       "      'source': 'duo',\n",
       "      'term_id': 'DUO:0000007'},\n",
       "     {'enumeration': 'not for profit, non commercial user only',\n",
       "      'source': 'duo',\n",
       "      'term_id': 'DUO:0000018'},\n",
       "     {'enumeration': 'ethics approval required',\n",
       "      'source': 'duo',\n",
       "      'term_id': 'DUO:0000021'},\n",
       "     {'enumeration': 'user specific restriction',\n",
       "      'source': 'duo',\n",
       "      'term_id': 'DUO:0000026'}]},\n",
       "   'termDef': [{'term': 'consent_codes'}],\n",
       "   'type': 'array'},\n",
       "  'patient_id': {'description': 'De-identified unique identifier for the patient/subject',\n",
       "   'type': 'string'},\n",
       "  'projects': {'anyOf': [{'items': {'additionalProperties': True,\n",
       "      'properties': {'code': {'type': 'string'},\n",
       "       'id': {'pattern': '^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$',\n",
       "        'term': {'description': 'A 128-bit identifier. Depending on the mechanism used to generate it, it is either guaranteed to be different from all other UUIDs/GUIDs generated until 3400 AD or extremely likely to be different. Its relatively small size lends itself well to sorting, ordering, and hashing of all sorts, storing in databases, simple allocation, and ease of programming in general.\\n',\n",
       "         'termDef': {'cde_id': 'C54100',\n",
       "          'cde_version': None,\n",
       "          'source': 'NCIt',\n",
       "          'term': 'Universally Unique Identifier',\n",
       "          'term_url': 'https://ncit.nci.nih.gov/ncitbrowser/ConceptReport.jsp?dictionary=NCI_Thesaurus&version=16.02d&ns=NCI_Thesaurus&code=C54100'}},\n",
       "        'type': 'string'}},\n",
       "      'type': 'object',\n",
       "      'maxItems': 1,\n",
       "      'minItems': 1},\n",
       "     'type': 'array'},\n",
       "    {'additionalProperties': True,\n",
       "     'properties': {'code': {'type': 'string'},\n",
       "      'id': {'pattern': '^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$',\n",
       "       'term': {'description': 'A 128-bit identifier. Depending on the mechanism used to generate it, it is either guaranteed to be different from all other UUIDs/GUIDs generated until 3400 AD or extremely likely to be different. Its relatively small size lends itself well to sorting, ordering, and hashing of all sorts, storing in databases, simple allocation, and ease of programming in general.\\n',\n",
       "        'termDef': {'cde_id': 'C54100',\n",
       "         'cde_version': None,\n",
       "         'source': 'NCIt',\n",
       "         'term': 'Universally Unique Identifier',\n",
       "         'term_url': 'https://ncit.nci.nih.gov/ncitbrowser/ConceptReport.jsp?dictionary=NCI_Thesaurus&version=16.02d&ns=NCI_Thesaurus&code=C54100'}},\n",
       "       'type': 'string'}},\n",
       "     'type': 'object'}]}},\n",
       " 'required': ['type', 'submitter_id', 'projects', 'patient_id'],\n",
       " 'submittable': True,\n",
       " 'systemProperties': ['id',\n",
       "  'project_id',\n",
       "  'state',\n",
       "  'created_datetime',\n",
       "  'updated_datetime'],\n",
       " 'title': 'Subject',\n",
       " 'type': 'object',\n",
       " 'uniqueKeys': [['id'], ['project_id', 'submitter_id']],\n",
       " 'validators': None}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resolved_schema = ResolveSchema(\"../schema/gen3_test_schema.json\")\n",
    "resolved_schema.return_resolved_schema('subject')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$schema': 'http://json-schema.org/draft-04/schema#',\n",
       " 'additionalProperties': False,\n",
       " 'category': 'biospecimen',\n",
       " 'description': \"Biospecimen information that links subjects to samples including sample's provider and source.\",\n",
       " 'id': 'sample',\n",
       " 'links': [{'backref': 'samples',\n",
       "   'label': 'taken_from',\n",
       "   'multiplicity': 'many_to_one',\n",
       "   'name': 'subjects',\n",
       "   'required': True,\n",
       "   'target_type': 'subject'}],\n",
       " 'namespace': 'https://data.test.biocommons.org.au/',\n",
       " 'program': '*',\n",
       " 'project': '*',\n",
       " 'properties': {'created_datetime': {'oneOf': [{'format': 'date-time',\n",
       "     'type': 'string'},\n",
       "    {'type': 'null'}],\n",
       "   'term': {'description': 'A combination of date and time of day in the form [-]CCYY-MM-DDThh:mm:ss[Z|(+|-)hh:mm]\\n'}},\n",
       "  'id': {'pattern': '^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$',\n",
       "   'term': {'description': 'A 128-bit identifier. Depending on the mechanism used to generate it, it is either guaranteed to be different from all other UUIDs/GUIDs generated until 3400 AD or extremely likely to be different. Its relatively small size lends itself well to sorting, ordering, and hashing of all sorts, storing in databases, simple allocation, and ease of programming in general.\\n',\n",
       "    'termDef': {'cde_id': 'C54100',\n",
       "     'cde_version': None,\n",
       "     'source': 'NCIt',\n",
       "     'term': 'Universally Unique Identifier',\n",
       "     'term_url': 'https://ncit.nci.nih.gov/ncitbrowser/ConceptReport.jsp?dictionary=NCI_Thesaurus&version=16.02d&ns=NCI_Thesaurus&code=C54100'}},\n",
       "   'type': 'string',\n",
       "   'systemAlias': 'node_id'},\n",
       "  'project_id': {'term': {'description': 'Unique ID for any specific defined piece of work that is undertaken or attempted to meet a single requirement.\\n'},\n",
       "   'type': 'string'},\n",
       "  'state': {'default': 'validated',\n",
       "   'downloadable': ['uploaded',\n",
       "    'md5summed',\n",
       "    'validating',\n",
       "    'validated',\n",
       "    'error',\n",
       "    'invalid',\n",
       "    'released'],\n",
       "   'oneOf': [{'enum': ['uploading',\n",
       "      'uploaded',\n",
       "      'md5summing',\n",
       "      'md5summed',\n",
       "      'validating',\n",
       "      'error',\n",
       "      'invalid',\n",
       "      'suppressed',\n",
       "      'redacted',\n",
       "      'live']},\n",
       "    {'enum': ['validated', 'submitted', 'released']}],\n",
       "   'public': ['live'],\n",
       "   'term': {'description': 'The current state of the object.\\n'}},\n",
       "  'submitter_id': {'description': 'A project-specific identifier for a node. This property is the calling card/nickname/alias for a unit of submission. It can be used in place of the UUID for identifying or recalling a node.\\n',\n",
       "   'type': ['string']},\n",
       "  'type': {'type': 'string'},\n",
       "  'updated_datetime': {'oneOf': [{'format': 'date-time', 'type': 'string'},\n",
       "    {'type': 'null'}],\n",
       "   'term': {'description': 'A combination of date and time of day in the form [-]CCYY-MM-DDThh:mm:ss[Z|(+|-)hh:mm]\\n'}},\n",
       "  'alternate_timepoint': {'description': 'If the data is not a baseline measurement, the timepoint name is defined here.',\n",
       "   'type': 'string'},\n",
       "  'baseline_timepoint': {'description': 'Does the data reflect a baseline measurement?',\n",
       "   'type': 'boolean'},\n",
       "  'freeze_thaw_cycles': {'description': 'Number of freeze thaw cycles conducted on the sample',\n",
       "   'type': 'integer'},\n",
       "  'sample_collection_method': {'description': 'How the sample was collected, e.g. blood draw',\n",
       "   'type': 'string'},\n",
       "  'sample_id': {'description': 'A unique sample identifier', 'type': 'string'},\n",
       "  'sample_in_preservation': {'description': 'The preservation method used for the sample',\n",
       "   'enum': ['cryopreserved',\n",
       "    'FFPE',\n",
       "    'fresh',\n",
       "    'OCT',\n",
       "    'snap Frozen',\n",
       "    'frozen',\n",
       "    'unknown',\n",
       "    'not reported',\n",
       "    'not allowed to collect'],\n",
       "   'enumDef': [{'enumeration': 'cryopreserved',\n",
       "     'source': 'BioData Catalyst DD'},\n",
       "    {'enumeration': 'FFPE', 'source': 'BioData Catalyst DD'},\n",
       "    {'enumeration': 'fresh', 'source': 'BioData Catalyst DD'},\n",
       "    {'enumeration': 'OCT', 'source': 'BioData Catalyst DD'},\n",
       "    {'enumeration': 'snap Frozen', 'source': 'BioData Catalyst DD'},\n",
       "    {'enumeration': 'frozen', 'source': 'BioData Catalyst DD'},\n",
       "    {'enumeration': 'unknown', 'source': 'BioData Catalyst DD'},\n",
       "    {'enumeration': 'not reported', 'source': 'BioData Catalyst DD'},\n",
       "    {'enumeration': 'not allowed to collect',\n",
       "     'source': 'BioData Catalyst DD'}]},\n",
       "  'sample_in_storage': {'description': 'Whether there is a sample in storage',\n",
       "   'enum': ['yes', 'no', 'unknown']},\n",
       "  'sample_provider': {'description': 'The name of collaborating institute that provided the sample',\n",
       "   'enum': ['Baker', 'USYD', 'UMELB', 'UQ']},\n",
       "  'sample_source': {'description': 'Uberon identifier, anatomical location as described by the Uber Anatomy Ontology (UBERON). (CMG, CCDG)',\n",
       "   'pattern': '^UBERON:[0-9]{7}$',\n",
       "   'type': 'string'},\n",
       "  'sample_storage_method': {'description': 'How the sample was/is stored',\n",
       "   'enum': ['not stored',\n",
       "    'ambient temperature',\n",
       "    'cut slide',\n",
       "    'fresh',\n",
       "    'frozen, -70C freezer',\n",
       "    'frozen, -150C freezer',\n",
       "    'frozen, liquid nitrogen',\n",
       "    'frozen, vapor phase',\n",
       "    'paraffin block',\n",
       "    'RNAlater, frozen',\n",
       "    'TRIzol, frozen'],\n",
       "   'enumDef': [{'enumeration': 'ambient temperature',\n",
       "     'source': 'https://schema.humancellatlas.org/module/biomaterial/6.1.1/preservation_storage',\n",
       "     'version': '6.1.1'},\n",
       "    {'enumeration': 'cut slide',\n",
       "     'source': 'https://schema.humancellatlas.org/module/biomaterial/6.1.1/preservation_storage',\n",
       "     'version': '6.1.1'},\n",
       "    {'enumeration': 'fresh',\n",
       "     'source': 'https://schema.humancellatlas.org/module/biomaterial/6.1.1/preservation_storage',\n",
       "     'version': '6.1.1'},\n",
       "    {'enumeration': 'frozen, -70C freezer',\n",
       "     'source': 'https://schema.humancellatlas.org/module/biomaterial/6.1.1/preservation_storage',\n",
       "     'version': '6.1.1'},\n",
       "    {'enumeration': 'frozen, -150C freezer',\n",
       "     'source': 'https://schema.humancellatlas.org/module/biomaterial/6.1.1/preservation_storage',\n",
       "     'version': '6.1.1'},\n",
       "    {'enumeration': 'frozen, liquid nitrogen',\n",
       "     'source': 'https://schema.humancellatlas.org/module/biomaterial/6.1.1/preservation_storage',\n",
       "     'version': '6.1.1'},\n",
       "    {'enumeration': 'frozen, vapor phase',\n",
       "     'source': 'https://schema.humancellatlas.org/module/biomaterial/6.1.1/preservation_storage',\n",
       "     'version': '6.1.1'},\n",
       "    {'enumeration': 'paraffin block',\n",
       "     'source': 'https://schema.humancellatlas.org/module/biomaterial/6.1.1/preservation_storage',\n",
       "     'version': '6.1.1'},\n",
       "    {'enumeration': 'RNAlater, frozen',\n",
       "     'source': 'https://schema.humancellatlas.org/module/biomaterial/6.1.1/preservation_storage',\n",
       "     'version': '6.1.1'},\n",
       "    {'enumeration': 'TRIzol, frozen',\n",
       "     'source': 'https://schema.humancellatlas.org/module/biomaterial/6.1.1/preservation_storage',\n",
       "     'version': '6.1.1'}]},\n",
       "  'sample_type': {'description': 'The type of sample, e.g. Whole Blood, Blood Cells, Frozen Blood',\n",
       "   'type': 'string'},\n",
       "  'storage_location': {'description': 'Where the sample is stored',\n",
       "   'enum': ['Baker', 'USYD', 'UMELB', 'UQ']},\n",
       "  'subjects': {'anyOf': [{'items': {'additionalProperties': True,\n",
       "      'properties': {'id': {'pattern': '^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$',\n",
       "        'term': {'description': 'A 128-bit identifier. Depending on the mechanism used to generate it, it is either guaranteed to be different from all other UUIDs/GUIDs generated until 3400 AD or extremely likely to be different. Its relatively small size lends itself well to sorting, ordering, and hashing of all sorts, storing in databases, simple allocation, and ease of programming in general.\\n',\n",
       "         'termDef': {'cde_id': 'C54100',\n",
       "          'cde_version': None,\n",
       "          'source': 'NCIt',\n",
       "          'term': 'Universally Unique Identifier',\n",
       "          'term_url': 'https://ncit.nci.nih.gov/ncitbrowser/ConceptReport.jsp?dictionary=NCI_Thesaurus&version=16.02d&ns=NCI_Thesaurus&code=C54100'}},\n",
       "        'type': 'string'},\n",
       "       'submitter_id': {'type': 'string'}},\n",
       "      'type': 'object',\n",
       "      'maxItems': 1,\n",
       "      'minItems': 1},\n",
       "     'type': 'array'},\n",
       "    {'additionalProperties': True,\n",
       "     'properties': {'id': {'pattern': '^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$',\n",
       "       'term': {'description': 'A 128-bit identifier. Depending on the mechanism used to generate it, it is either guaranteed to be different from all other UUIDs/GUIDs generated until 3400 AD or extremely likely to be different. Its relatively small size lends itself well to sorting, ordering, and hashing of all sorts, storing in databases, simple allocation, and ease of programming in general.\\n',\n",
       "        'termDef': {'cde_id': 'C54100',\n",
       "         'cde_version': None,\n",
       "         'source': 'NCIt',\n",
       "         'term': 'Universally Unique Identifier',\n",
       "         'term_url': 'https://ncit.nci.nih.gov/ncitbrowser/ConceptReport.jsp?dictionary=NCI_Thesaurus&version=16.02d&ns=NCI_Thesaurus&code=C54100'}},\n",
       "       'type': 'string'},\n",
       "      'submitter_id': {'type': 'string'}},\n",
       "     'type': 'object'}]}},\n",
       " 'required': ['type', 'submitter_id', 'subjects'],\n",
       " 'submittable': True,\n",
       " 'systemProperties': ['id',\n",
       "  'project_id',\n",
       "  'state',\n",
       "  'created_datetime',\n",
       "  'updated_datetime'],\n",
       " 'title': 'Sample',\n",
       " 'type': 'object',\n",
       " 'uniqueKeys': [['id'], ['project_id', 'submitter_id']],\n",
       " 'validators': None}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resolved_schema = ResolveSchema(\"../schema/gen3_test_schema.json\")\n",
    "def_schema = resolved_schema.return_schema('_definitions.yaml')\n",
    "terms_schema = resolved_schema.return_schema('_terms.yaml')\n",
    "sample_schema = resolved_schema.return_schema('sample.yaml')\n",
    "demographic_schema = resolved_schema.return_schema('demographic.yaml')\n",
    "\n",
    "\n",
    "def resolve_references(schema: dict, reference: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Takes a gen3 jsonschema draft 4 as a dictionary and recursively resolves any references using a reference schema which has no references.\n",
    "\n",
    "    Parameters:\n",
    "    - schema (dict): The JSON node to resolve references in.\n",
    "    - reference (dict): the schema containing the references\n",
    "\n",
    "    Returns:\n",
    "    - dict: The resolved JSON node with references resolved.\n",
    "    \"\"\"\n",
    "    \n",
    "    ref_input_content = reference\n",
    "\n",
    "\n",
    "    def resolve_node(node, manual_ref_content=ref_input_content):\n",
    "        if isinstance(node, dict):\n",
    "            if '$ref' in node:\n",
    "                ref_path = node['$ref']\n",
    "                ref_file, ref_key = ref_path.split('#')\n",
    "                ref_file = ref_file.strip()\n",
    "                ref_key = ref_key.strip('/')\n",
    "                # print(f'Resolving $ref: {ref_file}#{ref_key}')\n",
    "            \n",
    "                # if a reference file is in the reference, load the pre-defined reference, if no file exists, then use the schema itself as reference\n",
    "                if ref_file:\n",
    "                    ref_content = manual_ref_content\n",
    "                else:\n",
    "                    ref_content = schema\n",
    "                \n",
    "                \n",
    "                for part in ref_key.split('/'):\n",
    "                    ref_content = ref_content[part]\n",
    "\n",
    "                resolved_content = resolve_node(ref_content)\n",
    "                # Merge resolved content with the current node, excluding the $ref key\n",
    "                return {**resolved_content, **{k: resolve_node(v) for k, v in node.items() if k != '$ref'}}\n",
    "            else:\n",
    "                return {k: resolve_node(v) for k, v in node.items()}\n",
    "        elif isinstance(node, list):\n",
    "            return [resolve_node(item) for item in node]\n",
    "        else:\n",
    "            return node\n",
    "\n",
    "    return resolve_node(schema)\n",
    "\n",
    "def_resolved = resolve_references(def_schema, terms_schema)\n",
    "def_resolved\n",
    "resolve_references(sample_schema, def_resolved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_dict = {\n",
    "    resolved_schema.schema['demographic.yaml']['links']: resolved_schema.schema['demographic.yaml']['links']\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'backref': 'lipidomics_files',\n",
       "  'label': 'data_from',\n",
       "  'multiplicity': 'many_to_one',\n",
       "  'name': 'lipidomics_assays',\n",
       "  'required': False,\n",
       "  'target_type': 'lipidomics_assay'},\n",
       " {'backref': 'lipidomics_files',\n",
       "  'label': 'data_from',\n",
       "  'multiplicity': 'one_to_one',\n",
       "  'name': 'core_metadata_collections',\n",
       "  'required': False,\n",
       "  'target_type': 'core_metadata_collection'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resolved_schema.get_node_link('lipidomics_file.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for getting upstream downstream from links\n",
    "def find_upstream_downstream(links):\n",
    "    \"\"\"Takes a list of dictionaries or a single dictionary of links and returns the upstream and downstream nodes\"\"\"\n",
    "\n",
    "    # Ensure links is a list\n",
    "    if isinstance(links, dict):\n",
    "        links = [links]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for link in links:\n",
    "        backref = link.get(\"backref\")\n",
    "        target_type = link.get(\"target_type\")\n",
    "        \n",
    "        if not backref or not target_type:\n",
    "            print(\"Missing essential keys in link:\", link)\n",
    "            results.append((None, None))\n",
    "            continue\n",
    "        \n",
    "        # strip last s from name\n",
    "        if backref.endswith('s'):\n",
    "            backref = backref[:-1]\n",
    "\n",
    "        # Determine upstream/downstream logic\n",
    "        upstream = target_type\n",
    "        downstream = backref\n",
    "        \n",
    "        print(f\"Upstream: {upstream}, Downstream: {downstream}\")\n",
    "\n",
    "        results.append((upstream, downstream))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upstream: lipidomics_assay, Downstream: lipidomics_file\n",
      "Upstream: core_metadata_collection, Downstream: lipidomics_file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('lipidomics_assay', 'lipidomics_file'),\n",
       " ('core_metadata_collection', 'lipidomics_file')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link = resolved_schema.get_node_link('lipidomics_file.yaml')\n",
    "find_upstream_downstream(link)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
