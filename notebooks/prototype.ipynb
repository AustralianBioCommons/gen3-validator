{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolve Schema\n",
    "\n",
    "**input**\n",
    "```\n",
    "+schema_path: str\n",
    "```\n",
    "\n",
    "**methods**\n",
    "```\n",
    "+read_json(schema_path: str): dict\n",
    "+split_json(schema: dict): list\n",
    "+resolver(entity1: dict, entity2: dict): dict\n",
    "+resolve_defs(terms: dict, defs: dict) : dict\n",
    "+ node_order(schema: dict): list\n",
    "+resolve_nodes(nodeList: list, splitJsonList: list): list\n",
    "+recombine_nodes(resolvedList: list) : dict\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "class ResolveSchema:\n",
    "    \n",
    "    def __init__(self, schema_path: str):\n",
    "        \"\"\"\n",
    "        Initialize the ResolveSchema class.\n",
    "\n",
    "        Parameters:\n",
    "        - schema_path (str): The path to the JSON schema file.\n",
    "        \"\"\"\n",
    "        self.schema_path = schema_path\n",
    "        self.schema = self.read_json(self.schema_path)\n",
    "        self.nodes = self.get_nodes()\n",
    "        self.node_pairs = self.get_all_node_pairs()\n",
    "        self.node_order = self.get_node_order(edges=self.node_pairs)\n",
    "        self.schema_list = self.split_json()\n",
    "        self.schema_def = self.return_schema(\"_definitions.yaml\")\n",
    "        self.schema_term = self.return_schema(\"_terms.yaml\")\n",
    "        self.schema_def_resolved = self.resolve_references(self.schema_def, self.schema_term)\n",
    "        self.schema_list_resolved = self.resolve_all_references()\n",
    "        \n",
    "    def read_json(self, path: str) -> dict:\n",
    "        \"\"\"\n",
    "        Read a JSON file and return its contents as a dictionary.\n",
    "\n",
    "        Parameters:\n",
    "        - path (str): The path to the JSON file.\n",
    "\n",
    "        Returns:\n",
    "        - dict: The contents of the JSON file.\n",
    "        \"\"\"\n",
    "        with open(path) as f:\n",
    "            return json.load(f)\n",
    "    \n",
    "    def get_nodes(self) -> list:\n",
    "        \"\"\"\n",
    "        Retrieve all node names from the schema.\n",
    "\n",
    "        Returns:\n",
    "        - list: A list of node names.\n",
    "        \"\"\"\n",
    "        nodes = list(self.schema.keys())\n",
    "        return nodes\n",
    "    \n",
    "    def get_node_link(self, node_name: str) -> tuple:\n",
    "        \"\"\"\n",
    "        Retrieve the links and ID for a given node.\n",
    "\n",
    "        Parameters:\n",
    "        - node_name (str): The name of the node.\n",
    "\n",
    "        Returns:\n",
    "        - tuple: A tuple containing the node ID and its links.\n",
    "        \"\"\"\n",
    "        links = self.schema[node_name]['links']\n",
    "        node_id = self.schema[node_name]['id']\n",
    "        if 'subgroup' in links[0]:\n",
    "            return node_id, links[0]['subgroup']\n",
    "        else:\n",
    "            return node_id, links\n",
    "\n",
    "    def find_upstream_downstream(self, node_name: str) -> list:\n",
    "        \"\"\"\n",
    "        Takes a node name and returns the upstream and downstream nodes.\n",
    "\n",
    "        Parameters:\n",
    "        - node_name (str): The name of the node.\n",
    "\n",
    "        Returns:\n",
    "        - list: A list of tuples representing upstream and downstream nodes.\n",
    "        \"\"\"\n",
    "        node_id, links = self.get_node_link(node_name)\n",
    "        \n",
    "        # Ensure links is a list\n",
    "        if isinstance(links, dict):\n",
    "            links = [links]\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for link in links:\n",
    "            target_type = link.get(\"target_type\")\n",
    "            \n",
    "            if not node_id or not target_type:\n",
    "                print(\"Missing essential keys in link:\", link)\n",
    "                results.append((None, None))\n",
    "                continue\n",
    "\n",
    "            results.append((target_type, node_id))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def get_all_node_pairs(self, excluded_nodes=[\"_definitions.yaml\", \"_terms.yaml\", \"_settings.yaml\", \"program.yaml\"]) -> list:\n",
    "        \"\"\"\n",
    "        Retrieve all node pairs, excluding specified nodes.\n",
    "\n",
    "        Parameters:\n",
    "        - excluded_nodes (list): A list of node names to exclude.\n",
    "\n",
    "        Returns:\n",
    "        - list: A list of node pairs.\n",
    "        \"\"\"\n",
    "        node_pairs = []\n",
    "        for node in self.nodes:\n",
    "            if not node in excluded_nodes:\n",
    "                node_pairs.extend(self.find_upstream_downstream(node))\n",
    "            else:\n",
    "                continue\n",
    "        return node_pairs\n",
    "    \n",
    "    def get_node_order(self, edges: list) -> list:\n",
    "        \"\"\"\n",
    "        Determine the order of nodes based on their dependencies.\n",
    "\n",
    "        Parameters:\n",
    "        - edges (list): A list of tuples representing node dependencies.\n",
    "\n",
    "        Returns:\n",
    "        - list: A list of nodes in topological order.\n",
    "        \"\"\"\n",
    "        # Build graph representation\n",
    "        graph = defaultdict(list)\n",
    "        in_degree = defaultdict(int)\n",
    "\n",
    "        for upstream, downstream in edges:\n",
    "            graph[upstream].append(downstream)\n",
    "            in_degree[downstream] += 1\n",
    "            if upstream not in in_degree:\n",
    "                in_degree[upstream] = 0\n",
    "\n",
    "        # Perform Topological Sorting (Kahn's Algorithm)\n",
    "        sorted_order = []\n",
    "        zero_in_degree = deque([node for node in in_degree if in_degree[node] == 0])\n",
    "\n",
    "        while zero_in_degree:\n",
    "            node = zero_in_degree.popleft()\n",
    "            sorted_order.append(node)\n",
    "            \n",
    "            for neighbor in graph[node]:\n",
    "                in_degree[neighbor] -= 1\n",
    "                if in_degree[neighbor] == 0:\n",
    "                    zero_in_degree.append(neighbor)\n",
    "\n",
    "        # Ensure core_metadata_collection is last\n",
    "        sorted_order.remove(\"core_metadata_collection\")\n",
    "        sorted_order.append(\"core_metadata_collection\")\n",
    "\n",
    "        return sorted_order\n",
    "    \n",
    "    def split_json(self) -> list:\n",
    "        \"\"\"\n",
    "        Split the schema into a list of individual node schemas.\n",
    "\n",
    "        Returns:\n",
    "        - list: A list of node schemas.\n",
    "        \"\"\"\n",
    "        schema_list = []\n",
    "        for node in self.nodes:\n",
    "            schema_list.append(self.schema[node])\n",
    "        return schema_list\n",
    "    \n",
    "    def return_schema(self, target_id: str) -> dict:\n",
    "        \"\"\"\n",
    "        Retrieves the first dictionary from a list where the 'id' key matches the target_id.\n",
    "\n",
    "        Parameters:\n",
    "        - target_id (str): The value of the 'id' key to match.\n",
    "\n",
    "        Returns:\n",
    "        - dict: The dictionary that matches the target_id, or None if not found.\n",
    "        \"\"\"\n",
    "        if target_id.endswith('.yaml'):\n",
    "            target_id = target_id[:-5]\n",
    "        \n",
    "        result = next((item for item in self.schema_list if item.get('id') == target_id), None)\n",
    "        if result is None:\n",
    "            print(f\"{target_id} not found\")\n",
    "        return result\n",
    "    \n",
    "    def resolve_references(self, schema: dict, reference: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Takes a gen3 jsonschema draft 4 as a dictionary and recursively resolves any references using a reference schema which has no references.\n",
    "\n",
    "        Parameters:\n",
    "        - schema (dict): The JSON node to resolve references in.\n",
    "        - reference (dict): The schema containing the references.\n",
    "\n",
    "        Returns:\n",
    "        - dict: The resolved JSON node with references resolved.\n",
    "        \"\"\"\n",
    "        ref_input_content = reference\n",
    "\n",
    "        def resolve_node(node, manual_ref_content=ref_input_content):\n",
    "            if isinstance(node, dict):\n",
    "                if '$ref' in node:\n",
    "                    ref_path = node['$ref']\n",
    "                    ref_file, ref_key = ref_path.split('#')\n",
    "                    ref_file = ref_file.strip()\n",
    "                    ref_key = ref_key.strip('/')\n",
    "                \n",
    "                    # if a reference file is in the reference, load the pre-defined reference, if no file exists, then use the schema itself as reference\n",
    "                    if ref_file:\n",
    "                        ref_content = manual_ref_content\n",
    "                    else:\n",
    "                        ref_content = schema\n",
    "                    \n",
    "                    for part in ref_key.split('/'):\n",
    "                        ref_content = ref_content[part]\n",
    "\n",
    "                    resolved_content = resolve_node(ref_content)\n",
    "                    # Merge resolved content with the current node, excluding the $ref key\n",
    "                    return {**resolved_content, **{k: resolve_node(v) for k, v in node.items() if k != '$ref'}}\n",
    "                else:\n",
    "                    return {k: resolve_node(v) for k, v in node.items()}\n",
    "            elif isinstance(node, list):\n",
    "                return [resolve_node(item) for item in node]\n",
    "            else:\n",
    "                return node\n",
    "\n",
    "        return resolve_node(schema)\n",
    "    \n",
    "    def resolve_all_references(self) -> list:\n",
    "        \"\"\"\n",
    "        Resolves references in all other schema dictionaries using the resolved definitions schema.\n",
    "\n",
    "        Returns:\n",
    "        - list: A list of resolved schema dictionaries.\n",
    "        \"\"\"\n",
    "        resolved_schema_list = []\n",
    "        for node in self.nodes:\n",
    "            \n",
    "            if node == \"_definitions.yaml\" or node == \"_terms.yaml\":\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                resolved_schema = self.resolve_references(self.schema[node], self.schema_def_resolved)\n",
    "                resolved_schema_list.append(resolved_schema)\n",
    "                print(f\"Resolved {node}\")\n",
    "            except KeyError as e:\n",
    "                print(f\"Error resolving {node}: Missing key {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error resolving {node}: {e}\")\n",
    "                \n",
    "        return resolved_schema_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved demographic.yaml\n",
      "Resolved project.yaml\n",
      "Resolved serum_marker_assay.yaml\n",
      "Resolved alignment_workflow.yaml\n",
      "Resolved imaging_file.yaml\n",
      "Resolved lipidomics_assay.yaml\n",
      "Resolved metabolomics_file.yaml\n",
      "Resolved acknowledgement.yaml\n",
      "Resolved medical_history.yaml\n",
      "Resolved _settings.yaml\n",
      "Resolved blood_pressure_test.yaml\n",
      "Resolved genomics_assay.yaml\n",
      "Resolved variant_file.yaml\n",
      "Resolved program.yaml\n",
      "Resolved serum_marker_file.yaml\n",
      "Resolved proteomics_assay.yaml\n",
      "Resolved sample.yaml\n",
      "Resolved unaligned_reads_file.yaml\n",
      "Resolved aligned_reads_index_file.yaml\n",
      "Resolved variant_workflow.yaml\n",
      "Resolved proteomics_file.yaml\n",
      "Resolved exposure.yaml\n",
      "Resolved metabolomics_assay.yaml\n",
      "Resolved lipidomics_mapping_file.yaml\n",
      "Resolved lipidomics_file.yaml\n",
      "Resolved aligned_reads_file.yaml\n",
      "Resolved lab_result.yaml\n",
      "Resolved medication.yaml\n",
      "Resolved publication.yaml\n",
      "Resolved subject.yaml\n",
      "Resolved core_metadata_collection.yaml\n"
     ]
    }
   ],
   "source": [
    "resolved_schema = ResolveSchema(\"../schema/gen3_test_schema.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$schema': 'http://json-schema.org/draft-04/schema#',\n",
       " 'additionalProperties': False,\n",
       " 'category': 'biospecimen',\n",
       " 'description': \"Biospecimen information that links subjects to samples including sample's provider and source.\",\n",
       " 'id': 'sample',\n",
       " 'links': [{'backref': 'samples',\n",
       "   'label': 'taken_from',\n",
       "   'multiplicity': 'many_to_one',\n",
       "   'name': 'subjects',\n",
       "   'required': True,\n",
       "   'target_type': 'subject'}],\n",
       " 'namespace': 'https://data.test.biocommons.org.au/',\n",
       " 'program': '*',\n",
       " 'project': '*',\n",
       " 'properties': {'created_datetime': {'oneOf': [{'format': 'date-time',\n",
       "     'type': 'string'},\n",
       "    {'type': 'null'}],\n",
       "   'term': {'description': 'A combination of date and time of day in the form [-]CCYY-MM-DDThh:mm:ss[Z|(+|-)hh:mm]\\n'}},\n",
       "  'id': {'pattern': '^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$',\n",
       "   'term': {'description': 'A 128-bit identifier. Depending on the mechanism used to generate it, it is either guaranteed to be different from all other UUIDs/GUIDs generated until 3400 AD or extremely likely to be different. Its relatively small size lends itself well to sorting, ordering, and hashing of all sorts, storing in databases, simple allocation, and ease of programming in general.\\n',\n",
       "    'termDef': {'cde_id': 'C54100',\n",
       "     'cde_version': None,\n",
       "     'source': 'NCIt',\n",
       "     'term': 'Universally Unique Identifier',\n",
       "     'term_url': 'https://ncit.nci.nih.gov/ncitbrowser/ConceptReport.jsp?dictionary=NCI_Thesaurus&version=16.02d&ns=NCI_Thesaurus&code=C54100'}},\n",
       "   'type': 'string',\n",
       "   'systemAlias': 'node_id'},\n",
       "  'project_id': {'term': {'description': 'Unique ID for any specific defined piece of work that is undertaken or attempted to meet a single requirement.\\n'},\n",
       "   'type': 'string'},\n",
       "  'state': {'default': 'validated',\n",
       "   'downloadable': ['uploaded',\n",
       "    'md5summed',\n",
       "    'validating',\n",
       "    'validated',\n",
       "    'error',\n",
       "    'invalid',\n",
       "    'released'],\n",
       "   'oneOf': [{'enum': ['uploading',\n",
       "      'uploaded',\n",
       "      'md5summing',\n",
       "      'md5summed',\n",
       "      'validating',\n",
       "      'error',\n",
       "      'invalid',\n",
       "      'suppressed',\n",
       "      'redacted',\n",
       "      'live']},\n",
       "    {'enum': ['validated', 'submitted', 'released']}],\n",
       "   'public': ['live'],\n",
       "   'term': {'description': 'The current state of the object.\\n'}},\n",
       "  'submitter_id': {'description': 'A project-specific identifier for a node. This property is the calling card/nickname/alias for a unit of submission. It can be used in place of the UUID for identifying or recalling a node.\\n',\n",
       "   'type': ['string']},\n",
       "  'type': {'type': 'string'},\n",
       "  'updated_datetime': {'oneOf': [{'format': 'date-time', 'type': 'string'},\n",
       "    {'type': 'null'}],\n",
       "   'term': {'description': 'A combination of date and time of day in the form [-]CCYY-MM-DDThh:mm:ss[Z|(+|-)hh:mm]\\n'}},\n",
       "  'alternate_timepoint': {'description': 'If the data is not a baseline measurement, the timepoint name is defined here.',\n",
       "   'type': 'string'},\n",
       "  'baseline_timepoint': {'description': 'Does the data reflect a baseline measurement?',\n",
       "   'type': 'boolean'},\n",
       "  'freeze_thaw_cycles': {'description': 'Number of freeze thaw cycles conducted on the sample',\n",
       "   'type': 'integer'},\n",
       "  'sample_collection_method': {'description': 'How the sample was collected, e.g. blood draw',\n",
       "   'type': 'string'},\n",
       "  'sample_id': {'description': 'A unique sample identifier', 'type': 'string'},\n",
       "  'sample_in_preservation': {'description': 'The preservation method used for the sample',\n",
       "   'enum': ['cryopreserved',\n",
       "    'FFPE',\n",
       "    'fresh',\n",
       "    'OCT',\n",
       "    'snap Frozen',\n",
       "    'frozen',\n",
       "    'unknown',\n",
       "    'not reported',\n",
       "    'not allowed to collect'],\n",
       "   'enumDef': [{'enumeration': 'cryopreserved',\n",
       "     'source': 'BioData Catalyst DD'},\n",
       "    {'enumeration': 'FFPE', 'source': 'BioData Catalyst DD'},\n",
       "    {'enumeration': 'fresh', 'source': 'BioData Catalyst DD'},\n",
       "    {'enumeration': 'OCT', 'source': 'BioData Catalyst DD'},\n",
       "    {'enumeration': 'snap Frozen', 'source': 'BioData Catalyst DD'},\n",
       "    {'enumeration': 'frozen', 'source': 'BioData Catalyst DD'},\n",
       "    {'enumeration': 'unknown', 'source': 'BioData Catalyst DD'},\n",
       "    {'enumeration': 'not reported', 'source': 'BioData Catalyst DD'},\n",
       "    {'enumeration': 'not allowed to collect',\n",
       "     'source': 'BioData Catalyst DD'}]},\n",
       "  'sample_in_storage': {'description': 'Whether there is a sample in storage',\n",
       "   'enum': ['yes', 'no', 'unknown']},\n",
       "  'sample_provider': {'description': 'The name of collaborating institute that provided the sample',\n",
       "   'enum': ['Baker', 'USYD', 'UMELB', 'UQ']},\n",
       "  'sample_source': {'description': 'Uberon identifier, anatomical location as described by the Uber Anatomy Ontology (UBERON). (CMG, CCDG)',\n",
       "   'pattern': '^UBERON:[0-9]{7}$',\n",
       "   'type': 'string'},\n",
       "  'sample_storage_method': {'description': 'How the sample was/is stored',\n",
       "   'enum': ['not stored',\n",
       "    'ambient temperature',\n",
       "    'cut slide',\n",
       "    'fresh',\n",
       "    'frozen, -70C freezer',\n",
       "    'frozen, -150C freezer',\n",
       "    'frozen, liquid nitrogen',\n",
       "    'frozen, vapor phase',\n",
       "    'paraffin block',\n",
       "    'RNAlater, frozen',\n",
       "    'TRIzol, frozen'],\n",
       "   'enumDef': [{'enumeration': 'ambient temperature',\n",
       "     'source': 'https://schema.humancellatlas.org/module/biomaterial/6.1.1/preservation_storage',\n",
       "     'version': '6.1.1'},\n",
       "    {'enumeration': 'cut slide',\n",
       "     'source': 'https://schema.humancellatlas.org/module/biomaterial/6.1.1/preservation_storage',\n",
       "     'version': '6.1.1'},\n",
       "    {'enumeration': 'fresh',\n",
       "     'source': 'https://schema.humancellatlas.org/module/biomaterial/6.1.1/preservation_storage',\n",
       "     'version': '6.1.1'},\n",
       "    {'enumeration': 'frozen, -70C freezer',\n",
       "     'source': 'https://schema.humancellatlas.org/module/biomaterial/6.1.1/preservation_storage',\n",
       "     'version': '6.1.1'},\n",
       "    {'enumeration': 'frozen, -150C freezer',\n",
       "     'source': 'https://schema.humancellatlas.org/module/biomaterial/6.1.1/preservation_storage',\n",
       "     'version': '6.1.1'},\n",
       "    {'enumeration': 'frozen, liquid nitrogen',\n",
       "     'source': 'https://schema.humancellatlas.org/module/biomaterial/6.1.1/preservation_storage',\n",
       "     'version': '6.1.1'},\n",
       "    {'enumeration': 'frozen, vapor phase',\n",
       "     'source': 'https://schema.humancellatlas.org/module/biomaterial/6.1.1/preservation_storage',\n",
       "     'version': '6.1.1'},\n",
       "    {'enumeration': 'paraffin block',\n",
       "     'source': 'https://schema.humancellatlas.org/module/biomaterial/6.1.1/preservation_storage',\n",
       "     'version': '6.1.1'},\n",
       "    {'enumeration': 'RNAlater, frozen',\n",
       "     'source': 'https://schema.humancellatlas.org/module/biomaterial/6.1.1/preservation_storage',\n",
       "     'version': '6.1.1'},\n",
       "    {'enumeration': 'TRIzol, frozen',\n",
       "     'source': 'https://schema.humancellatlas.org/module/biomaterial/6.1.1/preservation_storage',\n",
       "     'version': '6.1.1'}]},\n",
       "  'sample_type': {'description': 'The type of sample, e.g. Whole Blood, Blood Cells, Frozen Blood',\n",
       "   'type': 'string'},\n",
       "  'storage_location': {'description': 'Where the sample is stored',\n",
       "   'enum': ['Baker', 'USYD', 'UMELB', 'UQ']},\n",
       "  'subjects': {'anyOf': [{'items': {'additionalProperties': True,\n",
       "      'properties': {'id': {'pattern': '^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$',\n",
       "        'term': {'description': 'A 128-bit identifier. Depending on the mechanism used to generate it, it is either guaranteed to be different from all other UUIDs/GUIDs generated until 3400 AD or extremely likely to be different. Its relatively small size lends itself well to sorting, ordering, and hashing of all sorts, storing in databases, simple allocation, and ease of programming in general.\\n',\n",
       "         'termDef': {'cde_id': 'C54100',\n",
       "          'cde_version': None,\n",
       "          'source': 'NCIt',\n",
       "          'term': 'Universally Unique Identifier',\n",
       "          'term_url': 'https://ncit.nci.nih.gov/ncitbrowser/ConceptReport.jsp?dictionary=NCI_Thesaurus&version=16.02d&ns=NCI_Thesaurus&code=C54100'}},\n",
       "        'type': 'string'},\n",
       "       'submitter_id': {'type': 'string'}},\n",
       "      'type': 'object',\n",
       "      'maxItems': 1,\n",
       "      'minItems': 1},\n",
       "     'type': 'array'},\n",
       "    {'additionalProperties': True,\n",
       "     'properties': {'id': {'pattern': '^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$',\n",
       "       'term': {'description': 'A 128-bit identifier. Depending on the mechanism used to generate it, it is either guaranteed to be different from all other UUIDs/GUIDs generated until 3400 AD or extremely likely to be different. Its relatively small size lends itself well to sorting, ordering, and hashing of all sorts, storing in databases, simple allocation, and ease of programming in general.\\n',\n",
       "        'termDef': {'cde_id': 'C54100',\n",
       "         'cde_version': None,\n",
       "         'source': 'NCIt',\n",
       "         'term': 'Universally Unique Identifier',\n",
       "         'term_url': 'https://ncit.nci.nih.gov/ncitbrowser/ConceptReport.jsp?dictionary=NCI_Thesaurus&version=16.02d&ns=NCI_Thesaurus&code=C54100'}},\n",
       "       'type': 'string'},\n",
       "      'submitter_id': {'type': 'string'}},\n",
       "     'type': 'object'}]}},\n",
       " 'required': ['type', 'submitter_id', 'subjects'],\n",
       " 'submittable': True,\n",
       " 'systemProperties': ['id',\n",
       "  'project_id',\n",
       "  'state',\n",
       "  'created_datetime',\n",
       "  'updated_datetime'],\n",
       " 'title': 'Sample',\n",
       " 'type': 'object',\n",
       " 'uniqueKeys': [['id'], ['project_id', 'submitter_id']],\n",
       " 'validators': None}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resolved_schema = ResolveSchema(\"../schema/gen3_test_schema.json\")\n",
    "def_schema = resolved_schema.return_schema('_definitions.yaml')\n",
    "terms_schema = resolved_schema.return_schema('_terms.yaml')\n",
    "sample_schema = resolved_schema.return_schema('sample.yaml')\n",
    "demographic_schema = resolved_schema.return_schema('demographic.yaml')\n",
    "\n",
    "\n",
    "def resolve_references(schema: dict, reference: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Takes a gen3 jsonschema draft 4 as a dictionary and recursively resolves any references using a reference schema which has no references.\n",
    "\n",
    "    Parameters:\n",
    "    - schema (dict): The JSON node to resolve references in.\n",
    "    - reference (dict): the schema containing the references\n",
    "\n",
    "    Returns:\n",
    "    - dict: The resolved JSON node with references resolved.\n",
    "    \"\"\"\n",
    "    \n",
    "    ref_input_content = reference\n",
    "\n",
    "\n",
    "    def resolve_node(node, manual_ref_content=ref_input_content):\n",
    "        if isinstance(node, dict):\n",
    "            if '$ref' in node:\n",
    "                ref_path = node['$ref']\n",
    "                ref_file, ref_key = ref_path.split('#')\n",
    "                ref_file = ref_file.strip()\n",
    "                ref_key = ref_key.strip('/')\n",
    "                # print(f'Resolving $ref: {ref_file}#{ref_key}')\n",
    "            \n",
    "                # if a reference file is in the reference, load the pre-defined reference, if no file exists, then use the schema itself as reference\n",
    "                if ref_file:\n",
    "                    ref_content = manual_ref_content\n",
    "                else:\n",
    "                    ref_content = schema\n",
    "                \n",
    "                \n",
    "                for part in ref_key.split('/'):\n",
    "                    ref_content = ref_content[part]\n",
    "\n",
    "                resolved_content = resolve_node(ref_content)\n",
    "                # Merge resolved content with the current node, excluding the $ref key\n",
    "                return {**resolved_content, **{k: resolve_node(v) for k, v in node.items() if k != '$ref'}}\n",
    "            else:\n",
    "                return {k: resolve_node(v) for k, v in node.items()}\n",
    "        elif isinstance(node, list):\n",
    "            return [resolve_node(item) for item in node]\n",
    "        else:\n",
    "            return node\n",
    "\n",
    "    return resolve_node(schema)\n",
    "\n",
    "def_resolved = resolve_references(def_schema, terms_schema)\n",
    "def_resolved\n",
    "resolve_references(sample_schema, def_resolved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_dict = {\n",
    "    resolved_schema.schema['demographic.yaml']['links']: resolved_schema.schema['demographic.yaml']['links']\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'backref': 'lipidomics_files',\n",
       "  'label': 'data_from',\n",
       "  'multiplicity': 'many_to_one',\n",
       "  'name': 'lipidomics_assays',\n",
       "  'required': False,\n",
       "  'target_type': 'lipidomics_assay'},\n",
       " {'backref': 'lipidomics_files',\n",
       "  'label': 'data_from',\n",
       "  'multiplicity': 'one_to_one',\n",
       "  'name': 'core_metadata_collections',\n",
       "  'required': False,\n",
       "  'target_type': 'core_metadata_collection'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resolved_schema.get_node_link('lipidomics_file.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for getting upstream downstream from links\n",
    "def find_upstream_downstream(links):\n",
    "    \"\"\"Takes a list of dictionaries or a single dictionary of links and returns the upstream and downstream nodes\"\"\"\n",
    "\n",
    "    # Ensure links is a list\n",
    "    if isinstance(links, dict):\n",
    "        links = [links]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for link in links:\n",
    "        backref = link.get(\"backref\")\n",
    "        target_type = link.get(\"target_type\")\n",
    "        \n",
    "        if not backref or not target_type:\n",
    "            print(\"Missing essential keys in link:\", link)\n",
    "            results.append((None, None))\n",
    "            continue\n",
    "        \n",
    "        # strip last s from name\n",
    "        if backref.endswith('s'):\n",
    "            backref = backref[:-1]\n",
    "\n",
    "        # Determine upstream/downstream logic\n",
    "        upstream = target_type\n",
    "        downstream = backref\n",
    "        \n",
    "        print(f\"Upstream: {upstream}, Downstream: {downstream}\")\n",
    "\n",
    "        results.append((upstream, downstream))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upstream: lipidomics_assay, Downstream: lipidomics_file\n",
      "Upstream: core_metadata_collection, Downstream: lipidomics_file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('lipidomics_assay', 'lipidomics_file'),\n",
       " ('core_metadata_collection', 'lipidomics_file')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link = resolved_schema.get_node_link('lipidomics_file.yaml')\n",
    "find_upstream_downstream(link)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
